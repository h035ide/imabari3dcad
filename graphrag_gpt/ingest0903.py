from tree_sitter import Language, Parser
import tree_sitter_python as tspython

from pathlib import Path
import re
from typing import List, Dict, Optional, Tuple, Protocol, Union, Any
import shutil
import logging
import json
from datetime import datetime

from langchain_core.documents import Document
from langchain_neo4j import Neo4jGraph
from neo4j.exceptions import ServiceUnavailable
from langchain_neo4j.graphs.graph_document import GraphDocument, Node, Relationship
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

# tree-sitterã®Pythonç”¨ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
PY_LANGUAGE = Language(tspython.language())
parser = Parser(PY_LANGUAGE)

logger = logging.getLogger(__name__)


class IngestConfigProtocol(Protocol):
    @property
    def neo4j_uri(self) -> str: ...

    @property
    def neo4j_user(self) -> str: ...

    @property
    def neo4j_password(self) -> str: ...

    @property
    def neo4j_database(self) -> str: ...

    @property
    def api_document_dir(self) -> Union[str, Path]: ...

    @property
    def chroma_persist_directory(self) -> Union[str, Path]: ...

    @property
    def langchain_embedding_config(self) -> Dict[str, Any]: ...

    @property
    def openai_api_key(self) -> Optional[str]: ...


# EntityæŠ½å‡ºç”¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å®šç¾©
# ã‚°ãƒ©ãƒ•ã‚’ãƒªãƒƒãƒåŒ–ã™ã‚‹ãŸã‚ã«ã€APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ½å‡ºã™ã‚‹CADé–¢é€£ã®å°‚é–€ç”¨èªã‚’å®šç¾©
ENTITY_KEYWORDS = [
    "ãƒ—ãƒ¬ãƒ¼ãƒˆ",
    "ã‚½ãƒªãƒƒãƒ‰",
    "è¦ç´ ",
    "åº§æ¨™",
    "ç‚¹",
    "ç·š",
    "ã‚«ãƒ¼ãƒ–",
    "å¹³é¢",
    "èˆ¹æ®»",
    "éƒ¨æ",
    "ãƒ–ãƒ­ãƒƒã‚¯",
    "ãƒ‘ãƒãƒ«",
    "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ",
    "ãƒ‘ãƒ¼ãƒˆ",
    "ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ",
]


def _read_api_text(data_dir: Path) -> str:
    """api.txt ã‚’å€™è£œãƒ‘ã‚¹ã‹ã‚‰èª­ã¿è¾¼ã‚€"""
    api_txt_candidates = [
        data_dir / "api.txt",
        Path("api.txt"),
        Path("/mnt/data/api.txt"),
    ]
    for p in api_txt_candidates:
        if p.exists():
            return p.read_text(encoding="utf-8")
    raise FileNotFoundError(
        "api.txt ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¬¡ã®ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„: "
        f"{[str(p) for p in api_txt_candidates]}"
    )


def _read_api_arg_text(data_dir: Path) -> str:
    """api_arg.txt ã‚’å€™è£œãƒ‘ã‚¹ã‹ã‚‰èª­ã¿è¾¼ã‚€"""
    api_arg_txt_candidates = [
        data_dir / "api_arg.txt",
        Path("api_arg.txt"),
        Path("/mnt/data/api_arg.txt"),
    ]
    for p in api_arg_txt_candidates:
        if p.exists():
            return p.read_text(encoding="utf-8")
    raise FileNotFoundError(
        "api_arg.txt ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¬¡ã®ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„: "
        f"{[str(p) for p in api_arg_txt_candidates]}"
    )


def _read_script_files(data_dir: Path) -> List[Tuple[str, str]]:
    """data ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã® .py ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã™ã¹ã¦èª­ã¿è¾¼ã‚€"""
    script_files = []
    if not data_dir.exists():
        logger.warning(
            f"{data_dir} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ã®è§£æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
        )
        return []

    # data ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã® .py ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢
    for p in data_dir.glob("*.py"):
        if p.is_file():
            try:
                # (ãƒ•ã‚¡ã‚¤ãƒ«å, ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹) ã®ã‚¿ãƒ—ãƒ«ã‚’è¿½åŠ 
                script_files.append((p.name, p.read_text(encoding="utf-8")))
            except Exception as e:
                print(f"âš  ãƒ•ã‚¡ã‚¤ãƒ« {p.name} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                continue

    return script_files


def _normalize_text(text: str) -> str:
    """
    æ”¹è¡Œ/ã‚¿ãƒ–/ç©ºç™½ã®æºã‚Œã‚’æ­£è¦åŒ–ã€‚
    - Windowsç³»æ”¹è¡Œã‚’ \n ã«
    - è¡Œæœ«ã®ç©ºç™½é™¤å»
    - ã‚¿ãƒ–â†’åŠè§’ã‚¹ãƒšãƒ¼ã‚¹
    - é€£ç¶šç©ºç™½ï¼ˆNBSP, å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹å«ã‚€ï¼‰â†’åŠè§’ã‚¹ãƒšãƒ¼ã‚¹1å€‹
    - BOMé™¤å»
    """
    text = text.replace("\ufeff", "")  # BOM
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    text = "\n".join(line.rstrip() for line in text.split("\n"))
    text = text.replace("\t", " ")
    text = re.sub(r"[ \u00A0\u3000]+", " ", text)
    return text


def _to_object_id_from_header(header: str) -> str:
    """
    'â– Partã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¡ã‚½ãƒƒãƒ‰' â†’ 'Part'
    æœ«å°¾ã® 'ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ' ã‚„ 'ã®ãƒ¡ã‚½ãƒƒãƒ‰' ã‚’é©å®œè½ã¨ã—ã¦ Object åã‚’æŠ½å‡º
    """
    s = header.strip()
    s = re.sub(r"^â– ", "", s)
    s = s.replace("ã®ãƒ¡ã‚½ãƒƒãƒ‰", "")
    s = s.replace("ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ", "")
    return s.strip()


def _guess_return_type_from_desc(desc: str) -> str:
    """
    è¿”ã‚Šå€¤èª¬æ˜ã‹ã‚‰ãŠãŠã¾ã‹ã«å‹ã‚’æ¨å®šã€‚
    ãƒ»'ID' / 'Id' / 'è¦ç´ ID' å«ã‚€ â†’ 'ID'
    ãƒ»ãã‚Œä»¥å¤–ã¯ 'ä¸æ˜'
    """
    d = desc or ""
    if re.search(r"\bID\b", d, flags=re.IGNORECASE) or ("è¦ç´ ID" in d):
        return "ID"
    return "ä¸æ˜"


def _parse_api_specs(text: str) -> List[Dict[str, Any]]:
    """
    api.txt ã‹ã‚‰ä»¥ä¸‹ã®æ§‹é€ ã®é…åˆ—ã‚’è¿”ã™:
    [
      {
        "object": "Part",
        "title_jp": "èˆ¹æ®»ã®ãƒ—ãƒ¬ãƒ¼ãƒˆã‚½ãƒªãƒƒãƒ‰è¦ç´ ã‚’ä½œæˆã™ã‚‹",
        "name": "CreatePlate",
        "return_desc": "ä½œæˆã—ãŸã‚½ãƒªãƒƒãƒ‰è¦ç´ ã®ID",
        "return_type": "ID",
        "params": [
          {"name": "...", "type": "...", "description": "..."},
          ...
        ],
      },
      ...
    ]
    """
    lines = text.split("\n")
    closing_pat = re.compile(r"\)\s*;?(?:\s*//.*)?$")
    param_pat = re.compile(
        r"^([A-Za-z_][A-Za-z0-9_]*)\s*,?\s*//\s*([^:ï¼š]+)\s*[:ï¼š]\s*(.*)$"
    )
    method_start_pat = re.compile(r"^([A-Za-z_][A-Za-z0-9_]*)\s*\($")
    header_pat = re.compile(r"^â– .+ã®ãƒ¡ã‚½ãƒƒãƒ‰$")
    title_pat = re.compile(r"^ã€‡(.+)$")
    ret_pat = re.compile(r"^è¿”ã‚Šå€¤[:ï¼š]\s*(.+)$")

    current_object = None
    current_title = None
    current_return_desc = None
    collecting_params = False
    current_entry: Optional[Dict[str, Any]] = None
    entries: List[Dict[str, Any]] = []
    i = 0
    n = len(lines)
    while i < n:
        line = lines[i].strip()
        if header_pat.match(line):
            current_object = _to_object_id_from_header(line)
            current_title = None
            current_return_desc = None
            i += 1
            continue
        m_title = title_pat.match(line)
        if m_title:
            current_title = m_title.group(1).strip()
            i += 1
            if i < n:
                m_ret = ret_pat.match(lines[i].strip())
                if m_ret:
                    current_return_desc = m_ret.group(1).strip()
                    i += 1
            continue
        m_start = method_start_pat.match(line)
        if m_start:
            method_name = m_start.group(1)
            current_entry = {
                "object": current_object or "Object",
                "title_jp": current_title or "",
                "name": method_name,
                "return_desc": current_return_desc or "",
                "return_type": _guess_return_type_from_desc(current_return_desc or ""),
                "params": [],
            }
            collecting_params = True
            i += 1
            continue
        if collecting_params and current_entry is not None:
            pm = param_pat.match(line)
            if pm:
                pname, ptype, pdesc = pm.groups()
                current_entry["params"].append(
                    {"name": pname, "type": ptype.strip(), "description": pdesc.strip()}
                )
                if closing_pat.search(line):
                    entries.append(current_entry)
                    current_entry = None
                    collecting_params = False
                i += 1
                continue
            if closing_pat.search(line):
                idx_close = line.rfind(")")
                before = line[:idx_close]
                token = before.split(",")[-1].strip()
                token = re.sub(r"[;,\s]+$", "", token)
                comment = line.split("//", 1)[1].strip() if "//" in line else ""
                synth = f"{token} // {comment}" if comment else token
                pm2 = param_pat.match(synth)
                if pm2:
                    pname, ptype, pdesc = pm2.groups()
                    current_entry["params"].append(
                        {
                            "name": pname,
                            "type": ptype.strip(),
                            "description": pdesc.strip(),
                        }
                    )
                entries.append(current_entry)
                current_entry = None
                collecting_params = False
                i += 1
                continue
            i += 1
            continue
        i += 1
    return entries


def _parse_data_type_descriptions(text: str) -> Dict[str, str]:
    """
    api_arg.txt ã‚’è§£æã—ã€ãƒ‡ãƒ¼ã‚¿å‹åã¨ãã®èª¬æ˜ã®è¾æ›¸ã‚’è¿”ã™ã€‚
    ä¾‹: {"æ–‡å­—åˆ—": "é€šå¸¸ã®æ–‡å­—åˆ—", "æµ®å‹•å°æ•°ç‚¹": "é€šå¸¸ã®æ•°å€¤", ...}
    """
    descriptions = {}
    current_type = None
    current_desc_lines = []

    normalized_text = _normalize_text(text)

    for line in normalized_text.split("\n"):
        line = line.strip()
        if not line:
            continue

        if line.startswith("â– "):
            if current_type and current_desc_lines:
                descriptions[current_type] = "\n".join(current_desc_lines).strip()

            current_type = line.replace("â– ", "").strip()
            current_desc_lines = []
        elif current_type:
            current_desc_lines.append(line)

    if current_type and current_desc_lines:
        descriptions[current_type] = "\n".join(current_desc_lines).strip()

    return descriptions


# ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã®ãƒªãƒƒãƒåŒ–ã‚’è¡Œã†é–¢æ•°ã‚’ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°
def extract_triples_from_specs(
    api_text: str, type_descriptions: Dict[str, str]
) -> Tuple[List[Dict[str, Any]], Dict[str, Dict[str, Any]]]:
    """
    ä»•æ§˜ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒãƒ¼ãƒ‰/ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒˆãƒªãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    DataTypeãƒãƒ¼ãƒ‰ã«ã¯api_arg.txtã‹ã‚‰æŠ½å‡ºã—ãŸèª¬æ˜(description)ã‚’è¿½åŠ ã™ã‚‹ã€‚
    å¤‰æ›´ç‚¹: Conceptãƒãƒ¼ãƒ‰ã¨Entityãƒãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã€ã‚°ãƒ©ãƒ•ã‚’ãƒªãƒƒãƒåŒ–
    """
    entries = _parse_api_specs(api_text)

    triples: List[Dict[str, Any]] = []
    node_props: Dict[str, Dict[str, Any]] = {}

    def _clean_type_name(type_name: str) -> str:
        """'ç‚¹(2D)' -> 'ç‚¹', 'è¦ç´ (é…åˆ—)' -> 'è¦ç´ ' ã®ã‚ˆã†ã«å‹åã‹ã‚‰æ‹¬å¼§æ›¸ãã‚’å‰Šé™¤ã™ã‚‹"""
        return re.sub(r"\s*\(.+\)$", "", type_name).strip()

    def create_data_type_node(raw_type_name: str) -> str:
        """DataTypeãƒãƒ¼ãƒ‰ã®å®šç¾©ã‚’ä½œæˆã—ã€ã‚¯ãƒªãƒ¼ãƒ³ãªå‹åã‚’è¿”ã™ã€‚"""
        cleaned_type_name = _clean_type_name(raw_type_name)
        if cleaned_type_name not in node_props:
            properties = {"name": cleaned_type_name}
            description = type_descriptions.get(cleaned_type_name)
            if description:
                properties["description"] = description
            node_props[cleaned_type_name] = {
                "type": "DataType",
                "properties": properties,
            }
        return cleaned_type_name

    def _extract_concept_from_title(title: str) -> Optional[str]:
        """ãƒ¡ã‚½ãƒƒãƒ‰ã®æ—¥æœ¬èªèª¬æ˜ã‹ã‚‰ã€ç›®çš„ã¨ãªã‚‹ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’ç°¡æ˜“çš„ã«æŠ½å‡ºã™ã‚‹"""
        if not title:
            return None
        # ä¾‹: ã€Œèˆ¹æ®»ã®ãƒ—ãƒ¬ãƒ¼ãƒˆã‚½ãƒªãƒƒãƒ‰è¦ç´ ã‚’ä½œæˆã™ã‚‹ã€ -> ã€Œèˆ¹æ®»ãƒ—ãƒ¬ãƒ¼ãƒˆã‚½ãƒªãƒƒãƒ‰è¦ç´ ä½œæˆã€
        #      ã€Œåº§æ¨™ã‚’å–å¾—ã™ã‚‹ã€ -> ã€Œåº§æ¨™å–å¾—ã€
        verbs_to_remove = [
            "ã‚’ä½œæˆã™ã‚‹",
            "ã‚’å–å¾—ã™ã‚‹",
            "ã‚’è¨­å®šã™ã‚‹",
            "ã«ã™ã‚‹",
            "ã‚’è¿”ã™",
            "ã™ã‚‹",
        ]
        processed_title = title
        for verb in verbs_to_remove:
            if processed_title.endswith(verb):
                processed_title = processed_title[: -len(verb)]
                break
        return processed_title.replace("ã®", "") + "æ©Ÿèƒ½"  # ã‚³ãƒ³ã‚»ãƒ—ãƒˆåã‚’æ˜ç¢ºåŒ–

    def _find_entities_in_text(text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆå†…ã‹ã‚‰å®šç¾©æ¸ˆã¿ã®Entityã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¢ã™"""
        found_entities = set()
        for keyword in ENTITY_KEYWORDS:
            if keyword in text:
                found_entities.add(keyword)
        return list(found_entities)

    for e in entries:
        obj_name = e["object"] or "Object"
        method_name = e["name"]
        title_jp = e.get("title_jp", "")
        ret_desc = e.get("return_desc", "")
        ret_type_raw = e.get("return_type", "ä¸æ˜")
        params = e.get("params", [])

        # --- åŸºæœ¬çš„ãªãƒãƒ¼ãƒ‰å®šç¾© ---
        node_props.setdefault(
            obj_name, {"type": "Object", "properties": {"name": obj_name}}
        )
        node_props.setdefault(
            method_name,
            {
                "type": "Method",
                "properties": {"name": method_name, "description": title_jp},
            },
        )
        ret_node_id = f"{method_name}_ReturnValue"
        node_props.setdefault(
            ret_node_id,
            {"type": "ReturnValue", "properties": {"description": ret_desc}},
        )
        cleaned_ret_type = create_data_type_node(ret_type_raw)

        # --- åŸºæœ¬çš„ãªé–¢ä¿‚å®šç¾© ---
        triples.append(
            {
                "source": method_name,
                "source_type": "Method",
                "label": "BELONGS_TO",
                "target": obj_name,
                "target_type": "Object",
            }
        )
        triples.append(
            {
                "source": method_name,
                "source_type": "Method",
                "label": "HAS_RETURNS",
                "target": ret_node_id,
                "target_type": "ReturnValue",
            }
        )
        triples.append(
            {
                "source": ret_node_id,
                "source_type": "ReturnValue",
                "label": "HAS_TYPE",
                "target": cleaned_ret_type,
                "target_type": "DataType",
            }
        )

        # --- Conceptãƒãƒ¼ãƒ‰ã®è¿½åŠ  ---
        concept_name = _extract_concept_from_title(title_jp)
        if concept_name:
            node_props.setdefault(
                concept_name, {"type": "Concept", "properties": {"name": concept_name}}
            )
            triples.append(
                {
                    "source": method_name,
                    "source_type": "Method",
                    "label": "PERFORMS_ACTION",
                    "target": concept_name,
                    "target_type": "Concept",
                }
            )

        # --- Entityãƒãƒ¼ãƒ‰ã®è¿½åŠ  (Method) ---
        method_entities = _find_entities_in_text(title_jp)
        for entity_name in method_entities:
            node_props.setdefault(
                entity_name, {"type": "Entity", "properties": {"name": entity_name}}
            )
            triples.append(
                {
                    "source": method_name,
                    "source_type": "Method",
                    "label": "RELATES_TO",
                    "target": entity_name,
                    "target_type": "Entity",
                }
            )

        # --- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨é–¢é€£Entityã®å‡¦ç† ---
        for i, p in enumerate(params):
            pname = p.get("name") or "Param"
            ptype_raw = p.get("type") or "å‹"
            pdesc = p.get("description") or ""
            param_node_id = f"{method_name}_{pname}"

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¼ãƒ‰ã‚’å®šç¾©
            node_props.setdefault(
                param_node_id,
                {
                    "type": "Parameter",
                    "properties": {"name": pname, "description": pdesc, "order": i},
                },
            )
            cleaned_ptype = create_data_type_node(ptype_raw)

            # é–¢ä¿‚: Method -> Parameter, Parameter -> DataType
            triples.append(
                {
                    "source": method_name,
                    "source_type": "Method",
                    "label": "HAS_PARAMETER",
                    "target": param_node_id,
                    "target_type": "Parameter",
                }
            )
            triples.append(
                {
                    "source": param_node_id,
                    "source_type": "Parameter",
                    "label": "HAS_TYPE",
                    "target": cleaned_ptype,
                    "target_type": "DataType",
                }
            )

            # --- Entityãƒãƒ¼ãƒ‰ã®è¿½åŠ  (Parameter) ---
            param_entities = _find_entities_in_text(pdesc)
            for entity_name in param_entities:
                node_props.setdefault(
                    entity_name, {"type": "Entity", "properties": {"name": entity_name}}
                )
                triples.append(
                    {
                        "source": param_node_id,
                        "source_type": "Parameter",
                        "label": "RELATES_TO",
                        "target": entity_name,
                        "target_type": "Entity",
                    }
                )

    return triples, node_props


def _extract_method_calls_from_script(script_text: str) -> List[Dict[str, str]]:
    """
    tree-sitter ã‚’ä½¿ã£ã¦ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰APIãƒ¡ã‚½ãƒƒãƒ‰ã®å‘¼ã³å‡ºã—ã‚’æŠ½å‡ºã™ã‚‹
    """
    tree = parser.parse(bytes(script_text, "utf8"))
    root_node = tree.root_node

    calls = []

    def find_calls(node):
        if node.type == "call":
            # `object.method()` ã®å½¢å¼ã‚’ç‰¹å®š
            function_node = node.child_by_field_name("function")
            if function_node and function_node.type == "attribute":
                obj_node = function_node.child_by_field_name("object")
                method_node = function_node.child_by_field_name("attribute")
                args_node = node.child_by_field_name("arguments")

                if obj_node and method_node and args_node:
                    call_info = {
                        "object_name": obj_node.text.decode("utf8"),
                        "method_name": method_node.text.decode("utf8"),
                        "arguments": args_node.text.decode("utf8"),
                        "full_text": node.text.decode("utf8"),
                    }
                    calls.append(call_info)

        for child in node.children:
            find_calls(child)

    find_calls(root_node)
    return calls


def extract_triples_from_script(
    script_path: str, script_text: str
) -> Tuple[List[Dict[str, Any]], Dict[str, Dict[str, Any]]]:
    """
    ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€ãƒãƒ¼ãƒ‰/ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒˆãƒªãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹
    """
    method_calls = _extract_method_calls_from_script(script_text)

    triples: List[Dict[str, Any]] = []
    node_props: Dict[str, Dict[str, Any]] = {}

    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆå…¨ä½“ã‚’è¡¨ã™ãƒãƒ¼ãƒ‰
    script_node_id = script_path
    node_props[script_node_id] = {
        "type": "ScriptExample",
        "properties": {"name": script_path},
    }

    prev_call_node_id = None

    for i, call in enumerate(method_calls):
        method_name = call["method_name"]
        call_node_id = f"{script_path}_call_{i}"

        # ãƒ¡ã‚½ãƒƒãƒ‰å‘¼ã³å‡ºã—ãƒãƒ¼ãƒ‰
        node_props[call_node_id] = {
            "type": "MethodCall",
            "properties": {"code": call["full_text"], "order": i},
        }

        # é–¢ä¿‚: ScriptExample -CONTAINS-> MethodCall
        triples.append(
            {
                "source": script_node_id,
                "source_type": "ScriptExample",
                "label": "CONTAINS",
                "target": call_node_id,
                "target_type": "MethodCall",
            }
        )

        # é–¢ä¿‚: MethodCall -CALLS-> Method (APIä»•æ§˜æ›¸ã§å®šç¾©ã•ã‚ŒãŸãƒ¡ã‚½ãƒƒãƒ‰)
        triples.append(
            {
                "source": call_node_id,
                "source_type": "MethodCall",
                "label": "CALLS",
                "target": method_name,
                "target_type": "Method",
            }
        )

        # é–¢ä¿‚: MethodCall -NEXT-> MethodCall (å‘¼ã³å‡ºã—é †åº)
        if prev_call_node_id:
            triples.append(
                {
                    "source": prev_call_node_id,
                    "source_type": "MethodCall",
                    "label": "NEXT",
                    "target": call_node_id,
                    "target_type": "MethodCall",
                }
            )

        prev_call_node_id = call_node_id

    return triples, node_props


def _triples_to_graph_documents(
    triples: List[Dict[str, Any]], node_props: Dict[str, Dict[str, Any]]
) -> List[GraphDocument]:
    """
    ãƒˆãƒªãƒ—ãƒ«ã¨ãƒãƒ¼ãƒ‰å±æ€§ã‹ã‚‰ GraphDocument ç¾¤ã‚’ä½œã‚‹
    """
    node_map: Dict[str, Node] = {}
    for node_id, meta in node_props.items():
        if node_id in node_map:
            existing_node = node_map[node_id]
            existing_node.properties.update(meta.get("properties", {}))
        else:
            ntype = meta["type"]
            props = meta.get("properties", {})
            node_map[node_id] = Node(id=node_id, type=ntype, properties=props)

    rels: List[Relationship] = []
    for t in triples:
        source_node = node_map.get(t["source"])
        if not source_node:
            source_node = Node(id=t["source"], type=t["source_type"])
            node_map[t["source"]] = source_node

        target_node = node_map.get(t["target"])
        if not target_node:
            target_node = Node(id=t["target"], type=t["target_type"])
            node_map[t["target"]] = target_node

        rels.append(
            Relationship(
                source=source_node, target=target_node, type=t["label"], properties={}
            )
        )

    doc = Document(page_content="API Spec and Example graph")
    gdoc = GraphDocument(nodes=list(node_map.values()), relationships=rels, source=doc)
    return [gdoc]


def _rebuild_graph_in_neo4j(
    graph_docs: List[GraphDocument], config: IngestConfigProtocol
) -> Tuple[int, int]:
    """
    Neo4j ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ã‹ã‚‰ GraphDocument ã‚’æŠ•å…¥ã™ã‚‹
    """
    # è¨­å®šã®ç¢ºèª
    if not all([config.neo4j_uri, config.neo4j_user, config.neo4j_password]):
        raise ValueError(
            "Neo4jæ¥ç¶šæƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )

    try:
        graph = Neo4jGraph(
            url=config.neo4j_uri,
            username=config.neo4j_user,
            password=config.neo4j_password,
            database=config.neo4j_database,
        )

        print("ğŸ§¹ Neo4jã®æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ä¸­...")
        delete_query = "MATCH (n) DETACH DELETE n"
        graph.query(delete_query)

        print("\nğŸš€ Neo4jã«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥ä¸­...")

        graph.add_graph_documents(graph_docs)

        res_nodes = graph.query("MATCH (n) RETURN count(n) AS c")
        res_rels = graph.query("MATCH ()-[r]->() RETURN count(r) AS c")
        return int(res_nodes[0]["c"]), int(res_rels[0]["c"])
    except Exception as e:
        print(f"âš  Neo4jæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        raise


def _export_neo4j_to_text(
    config: IngestConfigProtocol, out_dir: Path
) -> Tuple[Path, Path]:
    """Neo4jå†…ã®ãƒãƒ¼ãƒ‰/ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’JSONLã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã€‚

    nodes.jsonl: {id, labels, properties}
    relationships.jsonl: {id, type, start, end, properties}
    """
    if not all([config.neo4j_uri, config.neo4j_user, config.neo4j_password]):
        raise ValueError("Neo4jæ¥ç¶šæƒ…å ±ãŒæœªè¨­å®šã§ã™")

    graph = Neo4jGraph(
        url=config.neo4j_uri,
        username=config.neo4j_user,
        password=config.neo4j_password,
        database=config.neo4j_database,
    )

    out_dir.mkdir(parents=True, exist_ok=True)
    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    nodes_path = out_dir / f"nodes_{ts}.jsonl"
    rels_path = out_dir / f"relationships_{ts}.jsonl"

    # ãƒãƒ¼ãƒ‰ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆelementId ã‚’ä½¿ç”¨ï¼‰
    with nodes_path.open("w", encoding="utf-8") as f_nodes:
        result = graph.query(
            "MATCH (n) RETURN elementId(n) AS element_id, labels(n) AS labels, properties(n) AS props"
        )
        for row in result:
            rec = {
                "element_id": row["element_id"],
                "labels": row["labels"],
                "properties": row["props"],
            }
            f_nodes.write(json.dumps(rec, ensure_ascii=False) + "\n")

    # ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆelementId ã‚’ä½¿ç”¨ï¼‰
    with rels_path.open("w", encoding="utf-8") as f_rels:
        query = (
            "MATCH (a)-[r]->(b) "
            "RETURN elementId(r) AS element_id, type(r) AS type, "
            "elementId(a) AS start_element_id, elementId(b) AS end_element_id, properties(r) AS props"
        )
        result = graph.query(query)
        for row in result:
            rec = {
                "element_id": row["element_id"],
                "type": row["type"],
                "start_element_id": row["start_element_id"],
                "end_element_id": row["end_element_id"],
                "properties": row["props"],
            }
            f_rels.write(json.dumps(rec, ensure_ascii=False) + "\n")

    return nodes_path, rels_path


def _build_and_load_chroma(
    api_entries: List[Dict[str, Any]],
    script_files: List[Tuple[str, str]],
    config: IngestConfigProtocol
) -> None:
    """
    APIä»•æ§˜ã¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«DB (Chroma) ã‚’æ§‹ç¯‰ãƒ»æ°¸ç¶šåŒ–ã™ã‚‹
    """
    logger.info("ChromaDBã®ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆãƒ»ä¿å­˜ä¸­...")

    # OpenAI APIã‚­ãƒ¼ã®ç¢ºèª
    if not config.openai_api_key:
        logger.warning("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ChromaDBã®ä½œæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    chroma_persist_dir = Path(config.chroma_persist_directory)
    if chroma_persist_dir.exists():
        shutil.rmtree(chroma_persist_dir)
    chroma_persist_dir.mkdir(exist_ok=True)

    docs_for_vectorstore: List[Document] = []

    # 1. APIä»•æ§˜ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆ
    for entry in api_entries:
        content_parts = [
            f"ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ: {entry['object']}",
            f"ãƒ¡ã‚½ãƒƒãƒ‰å: {entry['name']}",
            f"èª¬æ˜: {entry['title_jp']}",
            f"è¿”ã‚Šå€¤: {entry['return_desc']}",
        ]
        if entry["params"]:
            param_texts = [
                f"- {p['name']} ({p['type']}): {p['description']}"
                for p in entry["params"]
            ]
            content_parts.append("ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:\n" + "\n".join(param_texts))

        content = "\n".join(content_parts)

        metadata = {
            "source": "api_spec",
            "object": entry["object"],
            "method_name": entry["name"],
        }
        docs_for_vectorstore.append(Document(page_content=content, metadata=metadata))

    # 2. ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆ
    for script_name, script_content in script_files:
        content = f"ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹: {script_name}\n\n```python\n{script_content}\n```"
        metadata = {
            "source": "script_example",
            "script_name": script_name,
        }
        docs_for_vectorstore.append(Document(page_content=content, metadata=metadata))

    try:
        embeddings = OpenAIEmbeddings(**config.langchain_embedding_config)  # type: ignore[arg-type]
        # è¨­å®šã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã«çµ±ä¸€ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯æ—¢å®šå€¤ã‚’ä½¿ç”¨ï¼‰
        collection_name = getattr(config, "chroma_collection_name", "api_documentation")
        Chroma.from_documents(
            documents=docs_for_vectorstore,
            embedding=embeddings,
            persist_directory=str(chroma_persist_dir),
            collection_name=collection_name,
        )
        logger.info(
            f"Chroma DB created and persisted with {len(docs_for_vectorstore)} documents at: "
            f"{chroma_persist_dir} (collection={collection_name})"
        )
    except Exception as e:
        msg = f"Chroma DBã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"
        logger.error(msg)


def _build_and_load_neo4j_from_docs(
    graph_docs: List[GraphDocument], config: IngestConfigProtocol
) -> None:
    """æº–å‚™æ¸ˆã¿ã® GraphDocument ã‚’ Neo4j ã«æŠ•å…¥ã™ã‚‹"""
    try:
        node_count, rel_count = _rebuild_graph_in_neo4j(graph_docs, config)
        logger.info(
            f"ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å†æ§‹ç¯‰ãŒå®Œäº†ã—ã¾ã—ãŸ: ãƒãƒ¼ãƒ‰={node_count}, ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—={rel_count}"
        )
    except ServiceUnavailable as se:
        logger.error(f"Neo4j ã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {se}")
        logger.error("Neo4jã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        logger.error(f"ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")


def _dump_preprocessed_artifacts(
    out_dir: Path,
    api_entries: List[Dict[str, Any]],
    type_descriptions: Dict[str, str],
    spec_triples: List[Dict[str, Any]],
    spec_node_props: Dict[str, Dict[str, Any]],
    script_triples: List[Dict[str, Any]],
    script_node_props: Dict[str, Dict[str, Any]],
) -> None:
    """å‰å‡¦ç†ã®æˆæœç‰©ã‚’JSONã§æ›¸ãå‡ºã™ã€‚

    - api_entries.json: _parse_api_specs ã®çµæœ
    - type_descriptions.json: _parse_data_type_descriptions ã®çµæœ
    - graph_specs.json: ä»•æ§˜ç”±æ¥ã®ãƒˆãƒªãƒ—ãƒ«/ãƒãƒ¼ãƒ‰
    - graph_scripts.json: ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”±æ¥ã®ãƒˆãƒªãƒ—ãƒ«/ãƒãƒ¼ãƒ‰
    - graph_all.json: çµ±åˆï¼ˆãƒˆãƒªãƒ—ãƒ«/ãƒãƒ¼ãƒ‰ï¼‰
    """
    out_dir.mkdir(parents=True, exist_ok=True)

    def _dump(obj: Any, name: str) -> None:
        (out_dir / name).write_text(
            json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8"
        )

    _dump(api_entries, "api_entries.json")
    _dump(type_descriptions, "type_descriptions.json")
    _dump({"triples": spec_triples, "nodes": spec_node_props}, "graph_specs.json")
    _dump({"triples": script_triples, "nodes": script_node_props}, "graph_scripts.json")

    # çµ±åˆãƒ“ãƒ¥ãƒ¼
    all_triples = list(spec_triples) + list(script_triples)
    all_nodes = dict(spec_node_props)
    all_nodes.update(script_node_props)
    _dump({"triples": all_triples, "nodes": all_nodes}, "graph_all.json")


def build_databases(config: IngestConfigProtocol) -> bool:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆConfigãƒ™ãƒ¼ã‚¹ï¼‰"""
    logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ã—ã¾ã™...")

    try:
        # Config ã® api_document_dir ã¯æ–‡å­—åˆ—ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ Path ã«æ­£è¦åŒ–
        data_dir = Path(config.api_document_dir)

        # --- 1. APIä»•æ§˜æ›¸ã¨å‹å®šç¾©ã®èª­ã¿è¾¼ã¿ãƒ»è§£æï¼ˆ1å›ã ã‘ï¼‰ ---
        logger.info("APIä»•æ§˜æ›¸ã‚’è§£æä¸­...")
        api_text = _normalize_text(_read_api_text(data_dir))
        api_arg_text = _read_api_arg_text(data_dir)
        type_descriptions = _parse_data_type_descriptions(api_arg_text)
        api_entries = _parse_api_specs(api_text)
        logger.info(f"{len(api_entries)}ä»¶ã®APIä»•æ§˜ã‚’è§£æã—ã¾ã—ãŸã€‚")

        # ä»•æ§˜ã‹ã‚‰ãƒˆãƒªãƒ—ãƒ«ç”Ÿæˆ
        spec_triples, spec_node_props = extract_triples_from_specs(
            api_text, type_descriptions
        )

        # --- 2. ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ã®èª­ã¿è¾¼ã¿ãƒ»è§£æï¼ˆ1å›ã ã‘ï¼‰ ---
        logger.info("ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ (data/*.py) ã‚’è§£æä¸­...")
        script_files = _read_script_files(data_dir)
        if script_files:
            logger.info(f"{len(script_files)}ä»¶ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
            all_script_triples: List[Dict[str, Any]] = []
            all_script_node_props: Dict[str, Dict[str, Any]] = {}
            for script_path, script_text in script_files:
                logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æä¸­: {script_path}")
                triples, node_props = extract_triples_from_script(
                    script_path, script_text
                )
                all_script_triples.extend(triples)
                all_script_node_props.update(node_props)
            script_triples = all_script_triples
            script_node_props = all_script_node_props
            logger.info(f"ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ã‹ã‚‰ãƒˆãƒªãƒ—ãƒ«ã‚’ç·è¨ˆ: {len(script_triples)} ä»¶")
        else:
            logger.warning("data ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«è§£æå¯¾è±¡ã® .py ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ã®è§£æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            script_triples, script_node_props = [], {}

        # --- 3. å‰å‡¦ç†çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ï¼ˆNeo4jæŠ•å…¥ã®å‰ï¼‰ ---
        try:
            dump_dir = Path(config.api_document_dir) / "preprocessed"
            _dump_preprocessed_artifacts(
                out_dir=dump_dir,
                api_entries=api_entries,
                type_descriptions=type_descriptions,
                spec_triples=spec_triples,
                spec_node_props=spec_node_props,
                script_triples=script_triples,
                script_node_props=script_node_props,
            )
            logger.info(f"å‰å‡¦ç†æˆæœç‰©ã‚’å‡ºåŠ›ã—ã¾ã—ãŸ: {dump_dir}")
        except Exception as e:
            logger.warning(f"å‰å‡¦ç†æˆæœç‰©ã®å‡ºåŠ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

        # --- 4. ãƒ‡ãƒ¼ã‚¿çµ±åˆ â†’ GraphDocument æ§‹ç¯‰ â†’ Neo4jæŠ•å…¥ ---
        logger.info("ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ä¸­...")
        all_triples = spec_triples + script_triples
        all_node_props = spec_node_props
        all_node_props.update(script_node_props)
        gdocs = _triples_to_graph_documents(all_triples, all_node_props)
        _build_and_load_neo4j_from_docs(gdocs, config)

        # --- 4. Neo4jã®å†…å®¹ã‚’ãƒ†ã‚­ã‚¹ãƒˆ(JSONL)ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ ---
        try:
            export_dir = Path(config.api_document_dir) / "preprocessed" / "neo4j_export"
            nodes_fp, rels_fp = _export_neo4j_to_text(config, export_dir)
            logger.info(f"Neo4jã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ: {nodes_fp.name}, {rels_fp.name}")
        except Exception as e:
            logger.warning(f"Neo4jã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

        # --- 5. ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ (Chroma) ã‚’æ§‹ç¯‰ï¼ˆèª­ã¿æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’å†åˆ©ç”¨ï¼‰ ---
        logger.info("ChromaDBæ§‹ç¯‰ãƒ—ãƒ­ã‚»ã‚¹")
        _build_and_load_chroma(api_entries, script_files, config)

        logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ãƒ—ãƒ­ã‚»ã‚¹ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        return True

    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logger.error("è¨­å®šã‚„ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return False


def main() -> None:
    """å¾“æ¥ã®äº’æ›æ€§ã®ãŸã‚ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆéæ¨å¥¨ï¼‰"""
    # å¾“æ¥ã®config.pyãƒ™ãƒ¼ã‚¹ã®å®Ÿè¡Œï¼ˆäº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰
    # import config

    class LegacyConfig:
        def __init__(self):
            self.neo4j_uri = os.getenv("NEO4J_URI", "bolt://127.0.0.1:7687")
            self.neo4j_user = os.getenv("NEO4J_USER", "neo4j")
            self.neo4j_password = os.getenv("NEO4J_PASSWORD", "password")
            self.neo4j_database = os.getenv("NEO4J_DATABASE", "neo4j")
            self.openai_api_key = os.getenv("OPENAI_API_KEY")
            self.api_document_dir = "data/src"
            self.chroma_persist_directory = "chroma_db_store"
            self.langchain_embedding_config = {
                "model": "text-embedding-3-small",
                "api_key": self.openai_api_key
            }

    legacy_config = LegacyConfig()
    build_databases(legacy_config)  # type: ignore[arg-type]


if __name__ == "__main__":
    # ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
    from dotenv import load_dotenv
    import os
    load_dotenv()
    main()
