import os
from dotenv import load_dotenv
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    Settings
)
from llama_index.core.callbacks import CallbackManager, TokenCountingHandler
from llama_index.llms.openai import OpenAI
import tiktoken
# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# ğŸ§  1. LLMã®æŒ‡å®š (gpt-4o)
llm = OpenAI(model="gpt-4o", temperature=0.5)
Settings.llm = llm

# ğŸ“Š 2. ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®æº–å‚™
token_counter = TokenCountingHandler(
    tokenizer=tiktoken.encoding_for_model("gpt-4o-mini").encode
)

callback_manager = CallbackManager([token_counter])
Settings.callback_manager = callback_manager

# 3. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
print("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
documents = SimpleDirectoryReader("./data/api_doc").load_data()

# 4. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ§‹ç¯‰ (å¼•æ•°ã‹ã‚‰service_contextã‚’å‰Šé™¤)
# Settingsã«è¨­å®šã—ãŸã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ãŒè‡ªå‹•ã§é©ç”¨ã•ã‚Œã¾ã™
print("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™...")
index = VectorStoreIndex.from_documents(documents)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰æ™‚ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¡¨ç¤º
print("\n--- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰æ™‚ã®ä½¿ç”¨çŠ¶æ³ ---")
print(f"Embeddingãƒˆãƒ¼ã‚¯ãƒ³æ•°: {token_counter.total_embedding_token_count}")
print("--------------------------------\n")

# ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
token_counter.reset_counts()

# 5. ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã®ä½œæˆ
print("ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½œæˆã—ã¦ã„ã¾ã™...")
query_engine = index.as_query_engine()

# 6. å•ã„åˆã‚ã›ã¨å¿œç­”ã®å–å¾—
print("å•ã„åˆã‚ã›ã‚’å®Ÿè¡Œã—ã¾ã™...")
question = "CreateVariable é–¢æ•°ã¯ä½•ã‚’è¿”ã—ã¾ã™ã‹ï¼Ÿ"
response = query_engine.query(question)

# å¿œç­”ã®è¡¨ç¤º
print("\n--- å¿œç­” ---")
print(f"è³ªå•: {question}")
print(f"å¿œç­”: {response}")
print("------------")

# 7. ã‚¯ã‚¨ãƒªå®Ÿè¡Œå¾Œã®APIä½¿ç”¨çŠ¶æ³ã‚’è¡¨ç¤º
print("\n--- ã‚¯ã‚¨ãƒªå®Ÿè¡Œæ™‚ã®ä½¿ç”¨çŠ¶æ³ ---")
print(f"LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {token_counter.prompt_llm_token_count}")
print(f"LLMå¿œç­”ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {token_counter.completion_llm_token_count}")
print(f"LLMåˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {token_counter.total_llm_token_count}")
print("---------------------------\n")