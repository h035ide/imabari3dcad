# This script runs specification tests for GraphRAG data retrieval from Neo4j database.
#
# ä½¿ç”¨æ–¹æ³•:
#   python test_rag_retrieval.py                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§docparserãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨
#   python test_rag_retrieval.py --database neo4j   # æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨
#   python test_rag_retrieval.py --save-docs        # ãƒ†ã‚¹ãƒˆçµæœã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã—ã¦ä¿å­˜
#   python test_rag_retrieval.py --output-dir reports # ã‚«ã‚¹ã‚¿ãƒ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š
#   python test_rag_retrieval.py test_patterns/     # ç‰¹å®šã®ãƒ†ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š
#   python test_rag_retrieval.py --help            # ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º
#
# ç’°å¢ƒå¤‰æ•°è¨­å®š (.envãƒ•ã‚¡ã‚¤ãƒ«):
#   NEO4J_URI=bolt://localhost:7687
#   NEO4J_USER=neo4j
#   NEO4J_PASSWORD=password
#   NEO4J_DATABASE=docparser (ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯docparser)

import os
import sys
import json
import argparse
import importlib.util
from datetime import datetime
from pathlib import Path
from neo4j import GraphDatabase
from dotenv import load_dotenv

# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å•é¡Œã‚’è§£æ±º
try:
    from .test_dsl import FunctionSpec, Param
except ImportError:
    sys.path.append(os.path.dirname(__file__))
    try:
        from test_dsl import FunctionSpec, Param
    except ImportError:
        print("Warning: test_dsl module not found. Some functionality may be limited.")
        FunctionSpec = None
        Param = None


class RagRetriever:
    def __init__(self, uri, user, password, database="docparser"):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))
        self.database = database
        self.test_results = []
        self.test_timestamp = None
        self.database_info = {}

    def close(self):
        self.driver.close()

    def _get_database_info(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åŸºæœ¬æƒ…å ±ã‚’å–å¾—"""
        try:
            with self.driver.session(database=self.database) as session:
                queries = {
                    "function_count": "MATCH (f:Function) RETURN count(f) as count",
                    "parameter_count": "MATCH (p:Parameter) RETURN count(p) as count",
                    "object_count": "MATCH (od:ObjectDefinition) RETURN count(od) as count"
                }
                
                results = {}
                for key, query in queries.items():
                    result = session.run(query).single()
                    results[key] = result['count'] if result else 0
                
                self.database_info = {
                    "database_name": self.database,
                    "function_count": results["function_count"],
                    "parameter_count": results["parameter_count"],
                    "object_definition_count": results["object_count"],
                    "test_timestamp": self.test_timestamp.isoformat() if self.test_timestamp else None
                }
        except Exception as e:
            print(f"Warning: Could not retrieve database statistics: {e}")
            self.database_info = {"database_name": self.database, "error": str(e)}

    def get_function_spec(self, function_name):
        """é–¢æ•°ã®å®Œå…¨ãªä»•æ§˜ã‚’å–å¾—"""
        with self.driver.session(database=self.database) as session:
            query = """
            MATCH (f:Function {name: $function_name})
            OPTIONAL MATCH (f)-[r:HAS_PARAMETER]->(p:Parameter)
            OPTIONAL MATCH (p)-[:HAS_TYPE]->(t)
            OPTIONAL MATCH (f)-[:RETURNS]->(ret)
            WITH f, ret, r, p, t
            ORDER BY r.position
            WITH f, ret, COLLECT(DISTINCT {
                name: p.name,
                description: p.description,
                position: r.position,
                type: t.name,
                is_object: 'ObjectDefinition' IN labels(t)
            }) as parameters
            RETURN
                f.name as name,
                f.description as description,
                parameters,
                ret.name as return_type,
                ret.description as return_description
            """
            result = session.run(query, function_name=function_name).single()

            if not result or not result['name']:
                print(f"Error: Function '{function_name}' not found.")
                return None

            spec = dict(result)
            # nullãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            spec['parameters'] = [p for p in spec['parameters'] if p['name'] is not None]
            # ä½ç½®ã§ã‚½ãƒ¼ãƒˆ
            spec['parameters'] = sorted(spec['parameters'], key=lambda x: x.get('position', 0))

            # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’å†å¸°çš„ã«å–å¾—
            for param in spec['parameters']:
                if param['is_object']:
                    param['properties'] = self._fetch_object_properties(session, param['type'])

            return spec

    def _fetch_object_properties(self, session, object_name):
        """ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå®šç¾©ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’å†å¸°çš„ã«å–å¾—"""
        query = """
        MATCH (od:ObjectDefinition {name: $object_name})
        OPTIONAL MATCH (od)-[:HAS_PROPERTY]->(p:Parameter)
        OPTIONAL MATCH (p)-[:HAS_TYPE]->(t)
        RETURN
            p.name as name,
            p.description as description,
            t.name as type,
            'ObjectDefinition' IN labels(t) as is_object
        """
        records = session.run(query, object_name=object_name).data()

        properties = []
        for record in records:
            if not record['name']:
                continue
            prop = dict(record)
            if prop['is_object']:
                prop['properties'] = self._fetch_object_properties(session, prop['type'])
            properties.append(prop)

        return properties

    def display_spec(self, spec):
        """å–å¾—ã—ãŸä»•æ§˜ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªå½¢å¼ã§è¡¨ç¤º"""
        if not spec:
            return

        print("\n--- API Specification ---")
        print(f"Function: {spec.get('name')}")
        print(f"  Description: {spec.get('description')}")
        print("-" * 25)

        print("Parameters:")
        if not spec.get('parameters'):
            print("  (None)")
        else:
            for param in spec['parameters']:
                self._display_parameter(param, indent_level=1)

        print("-" * 25)
        print("Returns:")
        print(f"  Type: {spec.get('return_type')}")
        print(f"  Description: {spec.get('return_description')}")
        print("-------------------------\n")

    def _display_parameter(self, param, indent_level=1):
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºï¼ˆå†å¸°çš„ï¼‰"""
        indent = "  " * indent_level
        print(f"{indent}- Name: {param.get('name')}")
        print(f"{indent}  Type: {param.get('type')}")
        print(f"{indent}  Description: {param.get('description')}")
        
        if 'properties' in param and param['properties']:
            print(f"{indent}  Properties:")
            for prop in param['properties']:
                self._display_parameter(prop, indent_level + 2)

    def run_test_cases_from_file(self, pattern_path):
        """ãƒ†ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã‚“ã§å®Ÿè¡Œ"""
        if FunctionSpec is None or Param is None:
            print(f"Warning: Skipping {pattern_path} - test_dsl module not available")
            self._add_test_result(os.path.basename(pattern_path), False, "test_dsl module not available")
            return
        
        # åˆå›å®Ÿè¡Œæ™‚ã®åˆæœŸåŒ–
        if not self.test_timestamp:
            self.test_timestamp = datetime.now()
            self._get_database_info()
        
        print(f"\nRunning tests from: {os.path.basename(pattern_path)}")
        
        try:
            module_name = os.path.splitext(os.path.basename(pattern_path))[0]
            spec = importlib.util.spec_from_file_location(module_name, pattern_path)
            test_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(test_module)
            test_cases = test_module.test_cases
        except (ImportError, AttributeError, FileNotFoundError) as e:
            details = f"Failed to load test cases from {pattern_path}: {e}"
            self._add_test_result(os.path.basename(pattern_path), False, details)
            return

        for case in test_cases:
            self._execute_single_test_case(case)

    def _execute_single_test_case(self, case):
        """å˜ä¸€ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å®Ÿè¡Œ"""
        test_name = case.get("test_name", "Unnamed Test")
        test_type = case.get("test_type", "positive")
        spec_obj = case.get("spec")

        if not spec_obj:
            self._add_test_result(test_name, False, "Test case is missing 'spec' definition.")
            return

        function_name = spec_obj.name
        expected_spec = spec_obj.to_dict()
        actual_spec = self.get_function_spec(function_name)
        
        if actual_spec is None:
            details = f"Function '{function_name}' not found in DB for test '{test_name}'."
            self._add_test_result(function_name, False, details, test_name)
            return

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆã‚’ä¸€è²«ã—ã¦ã‚½ãƒ¼ãƒˆ
        if actual_spec.get('parameters'):
            actual_spec['parameters'] = sorted(actual_spec['parameters'], key=lambda p: p.get('position', 0))

        is_match, details = self._compare_specs(expected_spec, actual_spec)

        # ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦çµæœã‚’åˆ¤å®š
        if test_type == 'positive':
            passed = is_match
            final_details = " | ".join(details) if not passed else "OK"
        else:  # negative
            passed = not is_match
            final_details = "Correctly identified mismatch." if passed else "Failed to identify expected mismatch."

        self._add_test_result(function_name, passed, final_details, test_name)

    def _compare_specs(self, expected, actual, path=""):
        """2ã¤ã®è¾æ›¸/ãƒªã‚¹ãƒˆæ§‹é€ ã‚’å†å¸°çš„ã«æ¯”è¼ƒã—ã¦å·®åˆ†ã‚’è¿”ã™"""
        errors = []

        if isinstance(expected, dict) and isinstance(actual, dict):
            # ã‚­ãƒ¼ã‚’ã‚½ãƒ¼ãƒˆã—ã¦é †åºã®é•ã„ã‚’å‡¦ç†
            expected_keys = sorted(expected.keys())
            actual_keys = sorted(actual.keys())

            if expected_keys != actual_keys:
                missing = set(expected_keys) - set(actual_keys)
                extra = set(actual_keys) - set(expected_keys)
                if missing:
                    errors.append(f"Missing keys at {path}: {missing}")
                if extra:
                    errors.append(f"Extra keys at {path}: {extra}")

            # å…±é€šã‚­ãƒ¼ã‚’æ¯”è¼ƒ
            common_keys = set(expected_keys) & set(actual_keys)
            for key in common_keys:
                new_path = f"{path}.{key}" if path else key
                if key in ['parameters', 'properties']:
                    continue  # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆã¯åˆ¥é€”å‡¦ç†
                errors.extend(self._compare_specs(expected[key], actual[key], new_path)[1])

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿/ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãƒªã‚¹ãƒˆã®ç‰¹åˆ¥å‡¦ç†
            for key in ['parameters', 'properties']:
                if key in common_keys:
                    new_path = f"{path}.{key}" if path else key
                    errors.extend(self._compare_lists(expected[key], actual[key], new_path))

        elif isinstance(expected, list) and isinstance(actual, list):
            errors.extend(self._compare_lists(expected, actual, path))

        elif expected != actual:
            errors.append(f"Value mismatch at {path}: Expected '{expected}', Got '{actual}'")

        return not errors, errors

    def _compare_lists(self, expected_list, actual_list, path):
        """ãƒªã‚¹ãƒˆã‚’æ¯”è¼ƒï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿/ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ç”¨ï¼‰"""
        errors = []
        
        if len(expected_list) != len(actual_list):
            errors.append(f"List length mismatch at {path}: Expected {len(expected_list)}, Got {len(actual_list)}")
            return errors

        # åå‰ã§ã‚­ãƒ¼åŒ–ã—ã¦æ¯”è¼ƒ
        expected_dict = {item['name']: item for item in expected_list}
        actual_dict = {item['name']: item for item in actual_list}

        if sorted(expected_dict.keys()) != sorted(actual_dict.keys()):
            missing = set(expected_dict.keys()) - set(actual_dict.keys())
            extra = set(actual_dict.keys()) - set(expected_dict.keys())
            if missing:
                errors.append(f"Missing items in list {path}: {missing}")
            if extra:
                errors.append(f"Extra items in list {path}: {extra}")
            return errors

        # å„ã‚¢ã‚¤ãƒ†ãƒ ã®è©³ç´°æ¯”è¼ƒ
        for name, expected_item in expected_dict.items():
            actual_item = actual_dict[name]
            errors.extend(self._compare_specs(expected_item, actual_item, f"{path}[{name}]")[1])

        return errors

    def get_all_function_names(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã™ã¹ã¦ã®é–¢æ•°åã‚’å–å¾—"""
        with self.driver.session(database=self.database) as session:
            query = "MATCH (f:Function) RETURN f.name as name ORDER BY f.name"
            records = session.run(query).data()
            return [r['name'] for r in records]

    def _add_test_result(self, function_name, passed, details, test_name=None):
        """ãƒ†ã‚¹ãƒˆçµæœã‚’è¿½åŠ """
        result = {
            "function": function_name,
            "passed": passed,
            "details": details
        }
        if test_name:
            result["test_name"] = test_name
        self.test_results.append(result)

    def save_results_as_markdown(self, output_dir="verification_reports"):
        """ãƒ†ã‚¹ãƒˆçµæœã‚’Markdownãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜"""
        output_path = Path(__file__).parent / output_dir
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp_str = self.test_timestamp.strftime("%Y%m%d_%H%M%S") if self.test_timestamp else "unknown"
        filename = f"rag_retrieval_test_{self.database}_{timestamp_str}.md"
        filepath = output_path / filename
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                self._write_markdown_header(f)
                self._write_database_info(f)
                self._write_test_summary(f)
                self._write_detailed_results(f)
                self._write_recommendations(f)
                self._write_footer(f)
            
            print(f"âœ… RAGãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
            return filepath
            
        except Exception as e:
            print(f"âŒ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return None

    def _write_markdown_header(self, f):
        """Markdownãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›¸ãè¾¼ã¿"""
        f.write(f"# RAG Retrieval ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"**ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å**: {self.database}\n")
        f.write(f"**ãƒ†ã‚¹ãƒˆæ—¥æ™‚**: {self.test_timestamp.strftime('%Y-%m-%d %H:%M:%S') if self.test_timestamp else 'Unknown'}\n")
        f.write(f"**ç”Ÿæˆå…ƒ**: test_rag_retrieval.py\n\n")

    def _write_database_info(self, f):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±ã‚’æ›¸ãè¾¼ã¿"""
        f.write("## ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±\n\n")
        if self.database_info:
            f.write(f"- **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å**: {self.database_info.get('database_name', 'Unknown')}\n")
            f.write(f"- **é–¢æ•°æ•°**: {self.database_info.get('function_count', 0)}å€‹\n")
            f.write(f"- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°**: {self.database_info.get('parameter_count', 0)}å€‹\n")
            f.write(f"- **ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå®šç¾©æ•°**: {self.database_info.get('object_definition_count', 0)}å€‹\n")

    def _write_test_summary(self, f):
        """ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼ã‚’æ›¸ãè¾¼ã¿"""
        f.write("\n## ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼\n\n")
        passed_count = sum(1 for r in self.test_results if r.get('passed'))
        total_tests = len(self.test_results)
        
        f.write(f"- **ç·ãƒ†ã‚¹ãƒˆæ•°**: {total_tests}\n")
        f.write(f"- **é€šéãƒ†ã‚¹ãƒˆæ•°**: {passed_count}\n")
        f.write(f"- **å¤±æ•—ãƒ†ã‚¹ãƒˆæ•°**: {total_tests - passed_count}\n")
        f.write(f"- **æˆåŠŸç‡**: {(passed_count/total_tests*100):.1f}%\n\n")
        
        overall_status = "âœ… å…¨ãƒ†ã‚¹ãƒˆé€šé" if passed_count == total_tests else "âŒ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆå¤±æ•—"
        f.write(f"**å…¨ä½“çµæœ**: {overall_status}\n")

    def _write_detailed_results(self, f):
        """è©³ç´°ãƒ†ã‚¹ãƒˆçµæœã‚’æ›¸ãè¾¼ã¿"""
        f.write("\n## è©³ç´°ãƒ†ã‚¹ãƒˆçµæœ\n\n")
        
        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        test_files = {}
        for result in self.test_results:
            test_file = result.get('function', 'Unknown')
            if test_file not in test_files:
                test_files[test_file] = []
            test_files[test_file].append(result)
        
        for test_file, results in test_files.items():
            f.write(f"### {test_file}\n\n")
            
            file_passed = sum(1 for r in results if r.get('passed'))
            file_total = len(results)
            
            f.write(f"**ãƒ•ã‚¡ã‚¤ãƒ«çµæœ**: {file_passed}/{file_total} é€šé\n\n")
            
            for result in results:
                status_icon = "âœ…" if result.get('passed') else "âŒ"
                test_name = result.get('test_name', 'Unnamed Test')
                details = result.get('details', 'No details')
                
                f.write(f"{status_icon} **{test_name}**\n")
                f.write(f"   - è©³ç´°: {details}\n\n")

    def _write_recommendations(self, f):
        """æ¨å¥¨äº‹é …ã‚’æ›¸ãè¾¼ã¿"""
        f.write("## æ¨å¥¨äº‹é …\n\n")
        passed_count = sum(1 for r in self.test_results if r.get('passed'))
        total_tests = len(self.test_results)
        
        if passed_count == total_tests:
            f.write("âœ… ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒé€šéã—ã¾ã—ãŸã€‚RAGæ¤œç´¢æ©Ÿèƒ½ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚\n")
        else:
            f.write("âŒ ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚ä»¥ä¸‹ã®ç‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š\n\n")
            for result in self.test_results:
                if not result.get('passed'):
                    f.write(f"- **{result.get('function', 'Unknown')}**: {result.get('details', 'No details')}\n")

    def _write_footer(self, f):
        """ãƒ•ãƒƒã‚¿ãƒ¼ã‚’æ›¸ãè¾¼ã¿"""
        f.write("\n---\n")
        f.write(f"*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ã«è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚*")

    def save_results_as_json(self, output_dir="verification_reports"):
        """ãƒ†ã‚¹ãƒˆçµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜"""
        output_path = Path(__file__).parent / output_dir
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp_str = self.test_timestamp.strftime("%Y%m%d_%H%M%S") if self.test_timestamp else "unknown"
        filename = f"rag_retrieval_test_{self.database}_{timestamp_str}.json"
        filepath = output_path / filename
        
        try:
            export_data = {
                "metadata": {
                    "database_name": self.database,
                    "test_timestamp": self.test_timestamp.isoformat() if self.test_timestamp else None,
                    "generator": "test_rag_retrieval.py",
                    "export_timestamp": datetime.now().isoformat()
                },
                "database_info": self.database_info,
                "test_results": self.test_results,
                "summary": self._generate_test_summary()
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… RAGãƒ†ã‚¹ãƒˆçµæœJSONã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
            return filepath
            
        except Exception as e:
            print(f"âŒ JSONä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return None

    def _generate_test_summary(self):
        """ãƒ†ã‚¹ãƒˆçµæœã®ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        passed_count = sum(1 for r in self.test_results if r.get('passed'))
        total_tests = len(self.test_results)
        
        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®çµ±è¨ˆ
        test_files = {}
        for result in self.test_results:
            test_file = result.get('function', 'Unknown')
            if test_file not in test_files:
                test_files[test_file] = {"passed": 0, "total": 0}
            test_files[test_file]["total"] += 1
            if result.get('passed'):
                test_files[test_file]["passed"] += 1
        
        return {
            "total_tests": total_tests,
            "passed_tests": passed_count,
            "failed_tests": total_tests - passed_count,
            "success_rate": (passed_count/total_tests*100) if total_tests > 0 else 0,
            "overall_status": "passed" if passed_count == total_tests else "failed",
            "test_file_summary": test_files
        }

    def run_basic_function_test(self, function_name):
        """åŸºæœ¬çš„ãªé–¢æ•°ä»•æ§˜ãƒ†ã‚¹ãƒˆï¼ˆtest_dslãŒãªãã¦ã‚‚å‹•ä½œï¼‰"""
        print(f"\nğŸ” Basic function test for: {function_name}")
        
        if not self.test_timestamp:
            self.test_timestamp = datetime.now()
            self._get_database_info()
        
        spec = self.get_function_spec(function_name)
        if spec:
            self.display_spec(spec)
            self._add_test_result(function_name, True, "Function specification retrieved successfully")
            print(f"âœ… Function '{function_name}' test passed")
        else:
            self._add_test_result(function_name, False, f"Function '{function_name}' not found in database")
            print(f"âŒ Function '{function_name}' test failed")

    def list_available_functions(self):
        """åˆ©ç”¨å¯èƒ½ãªé–¢æ•°ã®ä¸€è¦§ã‚’è¡¨ç¤º"""
        print("\nğŸ“‹ Available functions in database:")
        try:
            functions = self.get_all_function_names()
            if functions:
                for i, func_name in enumerate(functions, 1):
                    print(f"  {i:2d}. {func_name}")
                print(f"\nTotal: {len(functions)} functions")
            else:
                print("  No functions found in database")
        except Exception as e:
            print(f"  Error retrieving function list: {e}")

    def run_all_functions_basic_check(self):
        """å…¨é–¢æ•°ã®åŸºæœ¬çš„ãªãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ"""
        print(f"\nğŸ” Running comprehensive function check for all functions...")
        
        if not self.test_timestamp:
            self.test_timestamp = datetime.now()
            self._get_database_info()
        
        functions = self.get_all_function_names()
        if not functions:
            print("âŒ No functions found in database")
            return
        
        print(f"ğŸ“‹ Found {len(functions)} functions to check")
        print("=" * 60)
        
        for i, func_name in enumerate(functions, 1):
            print(f"\n[{i:3d}/{len(functions)}] Checking: {func_name}")
            print("-" * 40)
            
            try:
                spec = self.get_function_spec(func_name)
                if spec:
                    print(f"âœ… Function found")
                    print(f"   Description: {spec.get('description', 'No description')}")
                    print(f"   Parameters: {len(spec.get('parameters', []))}")
                    print(f"   Return Type: {spec.get('return_type', 'Unknown')}")
                    
                    self._add_test_result(func_name, True, 
                                        f"Function specification retrieved successfully - {len(spec.get('parameters', []))} parameters")
                else:
                    print(f"âŒ Function not found")
                    self._add_test_result(func_name, False, f"Function '{func_name}' not found in database")
                    
            except Exception as e:
                print(f"âŒ Error checking function: {e}")
                self._add_test_result(func_name, False, f"Error during check: {str(e)}")
        
        print("\n" + "=" * 60)
        print("ğŸ¯ Comprehensive function check completed!")
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        passed_count = sum(1 for r in self.test_results if r.get('passed'))
        total_count = len(self.test_results)
        print(f"ğŸ“Š Results: {passed_count}/{total_count} functions passed")
        
        return self.test_results


def main():
    parser = argparse.ArgumentParser(
        description="Run specification tests for GraphRAG data retrieval.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  python test_rag_retrieval.py                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§docparserãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨
  python test_rag_retrieval.py --database neo4j   # æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨
  python test_rag_retrieval.py --save-docs        # ãƒ†ã‚¹ãƒˆçµæœã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã—ã¦ä¿å­˜
  python test_rag_retrieval.py --output-dir reports # ã‚«ã‚¹ã‚¿ãƒ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š
  python test_rag_retrieval.py test_patterns/     # ç‰¹å®šã®ãƒ†ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š
  python test_rag_retrieval.py --function CreateSolid # ç‰¹å®šã®é–¢æ•°ã‚’ãƒ†ã‚¹ãƒˆ
  python test_rag_retrieval.py --list-functions   # åˆ©ç”¨å¯èƒ½ãªé–¢æ•°ä¸€è¦§ã‚’è¡¨ç¤º
  python test_rag_retrieval.py --all-functions    # å…¨é–¢æ•°ã®åŸºæœ¬çš„ãªãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ
        """
    )
    
    parser.add_argument("test_path", nargs='?', default=None, 
                      help="Path to a specific test file or directory.")
    parser.add_argument('--database', type=str, default='docparser',
                      help='ä½¿ç”¨ã™ã‚‹Neo4jãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: docparser)')
    parser.add_argument('--save-docs', action='store_true',
                      help='ãƒ†ã‚¹ãƒˆçµæœã‚’Markdownã¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜')
    parser.add_argument('--output-dir', type=str, default='verification_reports',
                      help='ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: verification_reports)')
    parser.add_argument('--function', type=str, metavar='FUNCTION_NAME',
                      help='ç‰¹å®šã®é–¢æ•°ã‚’ãƒ†ã‚¹ãƒˆï¼ˆtest_dslãŒãªãã¦ã‚‚å‹•ä½œï¼‰')
    parser.add_argument('--list-functions', action='store_true',
                      help='ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®åˆ©ç”¨å¯èƒ½ãªé–¢æ•°ä¸€è¦§ã‚’è¡¨ç¤º')
    parser.add_argument('--all-functions', action='store_true',
                      help='å…¨é–¢æ•°ã®åŸºæœ¬çš„ãªãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ')
    
    args = parser.parse_args()

    load_dotenv()
    NEO4J_URI = os.getenv("NEO4J_URI")
    NEO4J_USER = os.getenv("NEO4J_USER") or os.getenv("NEO4J_USERNAME")
    NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD")
    NEO4J_DATABASE = args.database

    if not all([NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD]):
        print("Error: NEO4J_URI, NEO4J_USER (or NEO4J_USERNAME), and NEO4J_PASSWORD must be set in the .env file.", file=sys.stderr)
        sys.exit(1)

    print(f"ğŸ” RAG Retrieval Test for database: {NEO4J_DATABASE}")
    if NEO4J_DATABASE == "docparser":
        print("  â†’ APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè§£æãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹docparserãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™")

    retriever = RagRetriever(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD, NEO4J_DATABASE)
    try:
        # é–¢æ•°ä¸€è¦§è¡¨ç¤º
        if args.list_functions:
            retriever.list_available_functions()
            return
        
        # ç‰¹å®šã®é–¢æ•°ãƒ†ã‚¹ãƒˆ
        if args.function:
            retriever.run_basic_function_test(args.function)
        # å…¨é–¢æ•°ã®åŸºæœ¬çš„ãªãƒã‚§ãƒƒã‚¯
        elif args.all_functions:
            retriever.run_all_functions_basic_check()
        # ãƒ†ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        elif args.test_path:
            test_paths = []
            if os.path.isdir(args.test_path):
                test_paths = [os.path.join(args.test_path, f) for f in os.listdir(args.test_path) 
                             if f.startswith('test_') and f.endswith('.py')]
            elif os.path.isfile(args.test_path):
                test_paths.append(args.test_path)
            
            if test_paths:
                for path in test_paths:
                    retriever.run_test_cases_from_file(path)
            else:
                print("No test files found.")
                return
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§test_patternsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
            patterns_dir = os.path.join(os.path.dirname(__file__), 'test_patterns')
            if os.path.isdir(patterns_dir):
                test_paths = [os.path.join(patterns_dir, f) for f in os.listdir(patterns_dir) 
                             if f.startswith('test_') and f.endswith('.py')]
                if test_paths:
                    for path in test_paths:
                        retriever.run_test_cases_from_file(path)
                else:
                    print("No test files found in test_patterns directory.")
                    return
            else:
                print("test_patterns directory not found.")
                return

        # æœ€çµ‚ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        if retriever.test_results:
            print("\n--- Final Test Summary ---")
            passed_count = sum(1 for r in retriever.test_results if r.get('passed'))
            total_tests = len(retriever.test_results)

            for result in retriever.test_results:
                if not result.get('passed'):
                    print(f"  âŒ FAILED - Test '{result.get('test_name', 'Unknown')}' in {result['function']}: {result['details']}")

            print(f"\nSummary: {passed_count}/{total_tests} tests passed.")

            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¿å­˜
            if args.save_docs:
                print(f"\nğŸ“„ ãƒ†ã‚¹ãƒˆçµæœã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã—ã¦ä¿å­˜ä¸­...")
                print(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.output_dir}")
                
                md_file = retriever.save_results_as_markdown(args.output_dir)
                json_file = retriever.save_results_as_json(args.output_dir)
                
                if md_file and json_file:
                    print(f"\nâœ… ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¿å­˜å®Œäº†:")
                    print(f"  ğŸ“ Markdown: {md_file}")
                    print(f"  ğŸ“Š JSON: {json_file}")
                else:
                    print(f"\nâš ï¸ ä¸€éƒ¨ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
        else:
            print("No tests were executed.")

    finally:
        retriever.close()


if __name__ == "__main__":
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    if project_root not in sys.path:
        sys.path.insert(0, project_root)
    main()
