"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç®¡ç†çµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã€ChromaDBãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€
æœ€é©åŒ–ã€ç›£è¦–æ©Ÿèƒ½ã‚’çµ±åˆçš„ã«æä¾›ã—ã¾ã™ã€‚
"""

import time
import json
from typing import Dict, Any, List, Optional
from pathlib import Path

try:
    from .performance_analyzer import PerformanceAnalyzer, PerformanceMetrics
    from .chromadb_benchmark import ChromaDBPerformanceBenchmark, BenchmarkResult
    from .chromadb_optimizer import ChromaDBOptimizer, OptimizationRecommendation
    from .performance_monitor import ChromaDBPerformanceMonitor, Alert
except ImportError:
    # ç›´æ¥å®Ÿè¡Œæ™‚ã®å¯¾å¿œ
    from performance_analyzer import PerformanceAnalyzer, PerformanceMetrics
    from chromadb_benchmark import ChromaDBPerformanceBenchmark, BenchmarkResult
    from chromadb_optimizer import ChromaDBOptimizer, OptimizationRecommendation
    from performance_monitor import ChromaDBPerformanceMonitor, Alert


class PerformanceManager:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç®¡ç†ã®çµ±åˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, collection=None, config: Dict[str, Any] = None):
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç®¡ç†ã®åˆæœŸåŒ–
        
        Args:
            collection: ChromaDBã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            config: è¨­å®šè¾æ›¸
        """
        self.collection = collection
        self.config = config or self._get_default_config()
        
        # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.code_analyzer = PerformanceAnalyzer()
        self.benchmark = ChromaDBPerformanceBenchmark(collection) if collection else None
        self.optimizer = ChromaDBOptimizer(collection) if collection else None
        self.monitor = ChromaDBPerformanceMonitor(
            collection, 
            alert_callback=self._handle_alert
        ) if collection else None
        
        # çµæœä¿å­˜ç”¨
        self.analysis_results = {}
        self.performance_reports = []

    def _get_default_config(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’å–å¾—"""
        return {
            "benchmark": {
                "collection_sizes": [100, 500, 1000],
                "batch_sizes": [10, 50, 100],
                "top_k_values": [1, 5, 10, 20],
                "dimension": 384
            },
            "monitoring": {
                "history_size": 1000,
                "alert_enabled": True,
                "continuous_monitoring": False,
                "monitoring_interval": 60
            },
            "analysis": {
                "include_file_analysis": True,
                "generate_reports": True,
                "export_results": True
            }
        }

    def analyze_code_performance(self, 
                               code_or_file: str, 
                               is_file: bool = False) -> Dict[str, PerformanceMetrics]:
        """
        ã‚³ãƒ¼ãƒ‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        
        Args:
            code_or_file: ã‚³ãƒ¼ãƒ‰æ–‡å­—åˆ—ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            is_file: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã©ã†ã‹
        
        Returns:
            åˆ†æçµæœã®è¾æ›¸
        """
        print("ã‚³ãƒ¼ãƒ‰ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã‚’é–‹å§‹...")
        
        if is_file:
            results = self.code_analyzer.analyze_file(code_or_file)
            analysis_type = f"file:{Path(code_or_file).name}"
        else:
            metrics = self.code_analyzer.analyze_code(code_or_file, "provided_code")
            results = {"provided_code": metrics}
            analysis_type = "code_snippet"
        
        # çµæœã‚’ä¿å­˜
        self.analysis_results[analysis_type] = {
            "timestamp": time.time(),
            "results": results,
            "summary": self._summarize_code_analysis(results)
        }
        
        print(f"åˆ†æå®Œäº†: {len(results)}å€‹ã®è¦ç´ ã‚’åˆ†æ")
        return results

    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ"""
        if not self.benchmark:
            return {"error": "ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ï¼ˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒæœªè¨­å®šï¼‰"}
        
        print("åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’é–‹å§‹...")
        
        config = self.config["benchmark"]
        results = self.benchmark.run_comprehensive_benchmark(
            collection_sizes=config["collection_sizes"],
            dimension=config["dimension"]
        )
        
        # çµæœã‚’ä¿å­˜
        benchmark_summary = {
            "timestamp": time.time(),
            "results": results,
            "recommendations": self._generate_benchmark_recommendations(results)
        }
        
        self.performance_reports.append({
            "type": "benchmark",
            "data": benchmark_summary
        })
        
        print("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†")
        return benchmark_summary

    def optimize_performance(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚’å®Ÿè¡Œ"""
        if not self.optimizer:
            return {"error": "æœ€é©åŒ–æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ï¼ˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒæœªè¨­å®šï¼‰"}
        
        print("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–åˆ†æã‚’é–‹å§‹...")
        
        # æœ€é©åŒ–æ¨å¥¨äº‹é …ã®å–å¾—
        recommendations = self.optimizer.get_optimization_recommendations()
        
        # æœ€é©åŒ–è¨­å®šã®ç”Ÿæˆ
        optimized_config = self.optimizer.create_optimized_collection_config(
            "optimized_collection"
        )
        
        # ãƒãƒƒãƒæœ€é©åŒ–ã®é©ç”¨
        batch_config = self.optimizer.apply_batch_optimization()
        
        optimization_results = {
            "timestamp": time.time(),
            "recommendations": recommendations,
            "optimized_config": optimized_config,
            "batch_config": batch_config,
            "optimization_report": self.optimizer.generate_optimization_report(recommendations)
        }
        
        self.performance_reports.append({
            "type": "optimization",
            "data": optimization_results
        })
        
        print(f"æœ€é©åŒ–åˆ†æå®Œäº†: {len(recommendations)}å€‹ã®æ¨å¥¨äº‹é …")
        return optimization_results

    def start_monitoring(self, continuous: bool = None) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’é–‹å§‹"""
        if not self.monitor:
            return {"error": "ç›£è¦–æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ï¼ˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒæœªè¨­å®šï¼‰"}
        
        if continuous is None:
            continuous = self.config["monitoring"]["continuous_monitoring"]
        
        if continuous:
            interval = self.config["monitoring"]["monitoring_interval"]
            self.monitor.start_continuous_monitoring(interval)
            print(f"ç¶™ç¶šç›£è¦–ã‚’é–‹å§‹ï¼ˆé–“éš”: {interval}ç§’ï¼‰")
        
        # åˆæœŸçµ±è¨ˆã®å–å¾—
        initial_stats = self.monitor.get_current_statistics()
        
        monitoring_info = {
            "timestamp": time.time(),
            "continuous_monitoring": continuous,
            "initial_statistics": initial_stats,
            "alert_rules": list(self.monitor.alert_rules.keys())
        }
        
        return monitoring_info

    def stop_monitoring(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’åœæ­¢"""
        if self.monitor:
            self.monitor.stop_continuous_monitoring()
            print("ç›£è¦–ã‚’åœæ­¢ã—ã¾ã—ãŸ")

    def get_performance_dashboard(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æƒ…å ±ã‚’å–å¾—"""
        dashboard = {
            "timestamp": time.time(),
            "system_status": "ç¨¼åƒä¸­"
        }
        
        # ã‚³ãƒ¼ãƒ‰åˆ†æçµæœã®ã‚µãƒãƒªãƒ¼
        if self.analysis_results:
            latest_analysis = max(
                self.analysis_results.values(),
                key=lambda x: x["timestamp"]
            )
            dashboard["code_analysis"] = latest_analysis["summary"]
        
        # ç›£è¦–çµ±è¨ˆ
        if self.monitor:
            try:
                current_stats = self.monitor.get_current_statistics()
                recent_alerts = self.monitor.get_alert_history(hours=24)
                trend_analysis = self.monitor.analyze_trends()
                
                dashboard["monitoring"] = {
                    "current_stats": current_stats,
                    "alert_count_24h": len(recent_alerts),
                    "trends": trend_analysis.get("trends", {}),
                    "recommendations": trend_analysis.get("recommendation", [])
                }
            except Exception as e:
                dashboard["monitoring"] = {"error": str(e)}
        
        # æœ€é©åŒ–çŠ¶æ³
        if self.optimizer:
            try:
                characteristics = self.optimizer.analyze_collection_characteristics()
                dashboard["collection_info"] = characteristics
            except Exception as e:
                dashboard["collection_info"] = {"error": str(e)}
        
        return dashboard

    def generate_comprehensive_report(self, output_dir: str = None) -> str:
        """åŒ…æ‹¬çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if output_dir is None:
            output_dir = "performance_reports"
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        timestamp = int(time.time())
        
        # å„ç¨®ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        reports = {}
        
        # 1. ã‚³ãƒ¼ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
        if self.analysis_results:
            code_report_path = output_path / f"code_analysis_{timestamp}.json"
            with open(code_report_path, 'w', encoding='utf-8') as f:
                json.dump(self.analysis_results, f, indent=2, ensure_ascii=False, default=str)
            reports["code_analysis"] = str(code_report_path)
        
        # 2. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆ
        if self.benchmark:
            benchmark_report_path = output_path / f"benchmark_{timestamp}.json"
            self.benchmark.export_results(str(benchmark_report_path))
            reports["benchmark"] = str(benchmark_report_path)
        
        # 3. æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ
        if self.optimizer:
            optimization_report_path = output_path / f"optimization_{timestamp}.json"
            self.optimizer.export_optimization_config(str(optimization_report_path))
            reports["optimization"] = str(optimization_report_path)
        
        # 4. ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆ
        if self.monitor:
            monitoring_report_path = output_path / f"monitoring_{timestamp}.json"
            self.monitor.export_monitoring_report(str(monitoring_report_path))
            reports["monitoring"] = str(monitoring_report_path)
        
        # 5. çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ
        comprehensive_report = {
            "generation_timestamp": time.time(),
            "report_files": reports,
            "dashboard": self.get_performance_dashboard(),
            "summary": self._generate_executive_summary()
        }
        
        comprehensive_report_path = output_path / f"comprehensive_report_{timestamp}.json"
        with open(comprehensive_report_path, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ: {comprehensive_report_path}")
        return str(comprehensive_report_path)

    def _summarize_code_analysis(self, results: Dict[str, PerformanceMetrics]) -> Dict[str, Any]:
        """ã‚³ãƒ¼ãƒ‰åˆ†æçµæœã®ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        if not results:
            return {}
        
        scores = [metrics.performance_score for metrics in results.values()]
        complexities = [metrics.time_complexity for metrics in results.values()]
        bottlenecks = [len(metrics.bottlenecks) for metrics in results.values()]
        
        return {
            "total_analyzed": len(results),
            "average_score": sum(scores) / len(scores),
            "common_complexities": list(set(complexities)),
            "total_bottlenecks": sum(bottlenecks),
            "needs_optimization": sum(1 for score in scores if score < 0.7)
        }

    def _generate_benchmark_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã«åŸºã¥ãæ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        if "results" in results:
            # æŒ¿å…¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®åˆ†æ
            insert_results = results["results"].get("bulk_insert", {})
            if insert_results:
                best_insert = max(
                    insert_results.values(),
                    key=lambda x: x.items_per_second if hasattr(x, 'items_per_second') else 0
                )
                if hasattr(best_insert, 'additional_metrics'):
                    batch_size = best_insert.additional_metrics.get("batch_size", 100)
                    recommendations.append(f"æ¨å¥¨ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size} (æœ€é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {best_insert.items_per_second:.1f} items/sec)")
            
            # æ¤œç´¢ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®åˆ†æ
            search_results = results["results"].get("search", {})
            if search_results:
                best_search = max(
                    search_results.values(),
                    key=lambda x: x.items_per_second if hasattr(x, 'items_per_second') else 0
                )
                if hasattr(best_search, 'additional_metrics'):
                    top_k = best_search.additional_metrics.get("top_k", 10)
                    recommendations.append(f"æ¨å¥¨æ¤œç´¢æ•°(top_k): {top_k} (æœ€é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {best_search.items_per_second:.1f} queries/sec)")
        
        if not recommendations:
            recommendations.append("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã«åŸºã¥ãå…·ä½“çš„ãªæ¨å¥¨äº‹é …ã¯ã‚ã‚Šã¾ã›ã‚“")
        
        return recommendations

    def _generate_executive_summary(self) -> Dict[str, Any]:
        """ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        summary = {
            "timestamp": time.time(),
            "overall_status": "æ­£å¸¸",
            "key_findings": [],
            "action_items": [],
            "performance_score": 0.8
        }
        
        # ã‚³ãƒ¼ãƒ‰åˆ†æã‹ã‚‰ã®ã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°
        if self.analysis_results:
            for analysis_type, data in self.analysis_results.items():
                analysis_summary = data.get("summary", {})
                needs_optimization = analysis_summary.get("needs_optimization", 0)
                total_analyzed = analysis_summary.get("total_analyzed", 1)
                
                if needs_optimization / total_analyzed > 0.3:
                    summary["key_findings"].append(f"{analysis_type}: 30%ä»¥ä¸Šã®é–¢æ•°ã§æœ€é©åŒ–ãŒå¿…è¦")
                    summary["action_items"].append("ã‚³ãƒ¼ãƒ‰æœ€é©åŒ–ã®è¨ˆç”»ç­–å®š")
        
        # ç›£è¦–ã‹ã‚‰ã®ã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°
        if self.monitor:
            recent_alerts = self.monitor.get_alert_history(hours=24)
            critical_alerts = [a for a in recent_alerts if a.severity == "Critical"]
            
            if critical_alerts:
                summary["key_findings"].append(f"24æ™‚é–“ä»¥å†…ã«ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¢ãƒ©ãƒ¼ãƒˆ{len(critical_alerts)}ä»¶")
                summary["action_items"].append("ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¢ãƒ©ãƒ¼ãƒˆã®åŸå› èª¿æŸ»ã¨å¯¾ç­–")
                summary["overall_status"] = "è¦æ³¨æ„"
        
        if not summary["key_findings"]:
            summary["key_findings"].append("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯è‰¯å¥½ãªçŠ¶æ…‹ã‚’ç¶­æŒ")
        
        if not summary["action_items"]:
            summary["action_items"].append("ç¶™ç¶šçš„ãªç›£è¦–ã‚’ç¶­æŒ")
        
        return summary

    def _handle_alert(self, alert: Alert):
        """ã‚¢ãƒ©ãƒ¼ãƒˆãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°"""
        print(f"ğŸš¨ ã‚¢ãƒ©ãƒ¼ãƒˆ: {alert.message}")
        
        # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¢ãƒ©ãƒ¼ãƒˆã®å ´åˆã¯è¿½åŠ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        if alert.severity == "Critical":
            print("ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¢ãƒ©ãƒ¼ãƒˆãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ç·Šæ€¥å¯¾å¿œãŒå¿…è¦ã§ã™ã€‚")


# ä½¿ç”¨ä¾‹ã®ãƒ‡ãƒ¢é–¢æ•°
def demo_performance_management():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç®¡ç†ã®ãƒ‡ãƒ¢"""
    print("=== ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç®¡ç†çµ±åˆãƒ‡ãƒ¢ ===")
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰
    sample_code = """
def inefficient_search(data, target):
    result = []
    for i in range(len(data)):
        for j in range(len(data)):
            if data[i] == target:
                result.append(i)
    return result
    """
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç®¡ç†ã®åˆæœŸåŒ–
    manager = PerformanceManager()
    
    print("\n1. ã‚³ãƒ¼ãƒ‰åˆ†æ")
    code_results = manager.analyze_code_performance(sample_code)
    for func_name, metrics in code_results.items():
        print(f"  {func_name}: ã‚¹ã‚³ã‚¢ {metrics.performance_score:.2f}, è¤‡é›‘åº¦ {metrics.time_complexity}")
    
    print("\n2. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    dashboard = manager.get_performance_dashboard()
    print(f"  ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹: {dashboard['system_status']}")
    if "code_analysis" in dashboard:
        print(f"  åˆ†ææ¸ˆã¿é–¢æ•°: {dashboard['code_analysis']['total_analyzed']}")
    
    print("\n3. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
    report_path = manager.generate_comprehensive_report()
    print(f"  ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å…ˆ: {report_path}")
    
    print("\nãƒ‡ãƒ¢å®Œäº†")


if __name__ == "__main__":
    demo_performance_management()