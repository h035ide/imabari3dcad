import sys
import os
import logging
import argparse
from pathlib import Path
from dotenv import load_dotenv
from main_helper_0905 import Config
from neo4j import GraphDatabase
import re
from typing import Optional

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š: LOG_LEVEL ç’°å¢ƒå¤‰æ•°ã§èª¿æ•´ï¼ˆæœªè¨­å®šã¯ INFOï¼‰
_log_level_name = os.getenv("LOG_LEVEL", "INFO").upper()
_log_level = getattr(logging, _log_level_name, logging.INFO)
logging.basicConfig(
    level=_log_level,
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
)


def run_nollm_doc(config: Config):
    """No LLM ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚’å®Ÿè¡Œï¼ˆingest0903.pyã‚’ä½¿ç”¨ï¼‰"""
    try:
        print("No LLM ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚’å®Ÿè¡Œä¸­...")
        # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸åŒ–ã—ãŸ graphrag_gpt ã‹ã‚‰é€šå¸¸ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§å®Ÿè¡Œ
        from graphrag_gpt.ingest0903 import build_databases
        success = build_databases(config)

        if success:
            print("âœ… No LLM ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
        else:
            print("âŒ No LLM ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")

        return success

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def run_llm_doc(config: Config):
    """LLM ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚’å®Ÿè¡Œ"""
    try:
        from doc_parser.neo4j_importer import import_to_neo4j

        # Neo4jã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        success = import_to_neo4j(
            uri=config.neo4j_uri,
            user=config.neo4j_user,
            password=config.neo4j_password,
            database=config.neo4j_database,
            file_path=config.parsed_api_result_def_file,
            use_def_file=True,
            config=config
        )

        return success
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def run_vectorization(config: Config):
    """LlamaIndexã‚’ä½¿ç”¨ã—ãŸåŠ¹ç‡çš„ãªãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
    try:
        from main_helper_0905 import (
            fetch_data_from_neo4j,
            ingest_data_to_chroma
        )

        print("Neo4jã‹ã‚‰APIãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        # Functionãƒãƒ¼ãƒ‰ã‚’å–å¾—ï¼ˆNeo4jã®ãƒ©ãƒ™ãƒ«ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
        records = fetch_data_from_neo4j(
            label="Function",
            db_name=config.neo4j_database,
            allow_missing_description=True,
            config=config
        )

        if not records:
            print("ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return False

        print(f"{len(records)}ä»¶ã®APIé–¢æ•°ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­...")
        # Chromaã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ä¿å­˜
        ingest_data_to_chroma(
            records=records,
            collection_name=config.chroma_collection_name,
            persist_dir=config.chroma_persist_directory,
            config=config
        )

        print("âœ… Chromaãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†")
        return True

    except Exception as e:
        print(f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def run_llamaindex_vectorization(config: Config):
    """LlamaIndexã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
    try:
        from llama_index.core import VectorStoreIndex, StorageContext
        from llama_index.vector_stores.chroma import ChromaVectorStore
        from llama_index.embeddings.openai import OpenAIEmbedding
        import chromadb

        # å‘¼ã³å‡ºã—å…ƒã‹ã‚‰å—ã‘å–ã£ãŸ Config ã‚’ä½¿ç”¨
        # è¨­å®šæƒ…å ±ã‚’è¡¨ç¤º
        print("ğŸ”§ LlamaIndexãƒ™ã‚¯ãƒˆãƒ«åŒ–è¨­å®š:")
        print(f"  LLMãƒ¢ãƒ‡ãƒ«: {config.llm_model}")
        print(f"  åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«: {config.embedding_model}")
        print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.embedding_batch_size}")
        print(f"  Chromaæ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {config.chroma_persist_directory}")
        print(f"  Chromaã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {config.chroma_collection_name}")
        if config.is_inference_model:
            print("  æ¨è«–ãƒ¢ãƒ‡ãƒ«è¨­å®š:")
            print(f"    Verbosity: {config.llm_verbosity}")
            print(f"    Reasoning Effort: {config.llm_reasoning_effort}")
        else:
            print("  æ¨™æº–ãƒ¢ãƒ‡ãƒ«è¨­å®š:")
            print(f"    Temperature: {config.llm_temperature}")

        # LlamaIndexã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šã‚’é¿ã‘ã‚‹ï¼‰
        embed_model = OpenAIEmbedding(
            **config.llamaindex_embedding_config
        )

        print("LlamaIndexã‚’ä½¿ç”¨ã—ãŸãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’é–‹å§‹...")

        # ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
        client = chromadb.PersistentClient(
            path=config.chroma_persist_directory
        )
        chroma_collection = client.get_or_create_collection(
            config.chroma_collection_name
        )

        # LlamaIndexã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        storage_context = StorageContext.from_defaults(
            vector_store=vector_store
        )

        # æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ï¼ˆembed_modelã‚’æ˜ç¤ºçš„ã«æŒ‡å®šï¼‰
        VectorStoreIndex.from_vector_store(
            vector_store,
            storage_context=storage_context,
            embed_model=embed_model
        )

        print("âœ… LlamaIndexãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†")
        print(f"  â†’ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {config.chroma_collection_name}")
        print(f"  â†’ æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: "
              f"{config.chroma_persist_directory}")

        return True

    except Exception as e:
        print(f"LlamaIndexãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def run_clear_database(
    config: Config,
    database: Optional[str] = None,
    force: bool = False,
):
    """Neo4jãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã®å®Ÿè¡Œ"""
    try:
        from doc_parser.clear_database import clear_database as _clear

        target_db = database or config.neo4j_database or "docparser"
        print(
            f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒªã‚¢ã‚’å®Ÿè¡Œã—ã¾ã™: db={target_db}, "
            f"force={force}"
        )
        _clear(database=target_db, force=force)
        return True
    except Exception as e:
        print(f"ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def run_qa_system(config: Config):
    """LangChainã§ãƒ©ãƒƒãƒ—ã—ãŸQAã‚·ã‚¹ãƒ†ãƒ ï¼ˆLangSmithã§ã‚¦ã‚©ãƒƒãƒå¯èƒ½ï¼‰"""
    try:
        from main_helper_0905 import build_langchain_wrapped_engines

        # LangChainã§ãƒ©ãƒƒãƒ—ã—ãŸã‚¨ãƒ³ã‚¸ãƒ³ã‚’å–å¾—
        engines = build_langchain_wrapped_engines(config)
        vector_search = engines['vector_search']
        graph_search = engines['graph_search']
        generate_response = engines['generate_response']

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è³ªå•ã‚’å…¥åŠ›ã—ã¦ã‚‚ã‚‰ã†
        print("\nLangChainçµ±åˆQAã‚·ã‚¹ãƒ†ãƒ ï¼ˆLangSmithå¯¾å¿œï¼‰")
        print("=" * 50)
        print("ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ :")
        print("  - ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢: ChromaDBï¼ˆæ„å‘³çš„é¡ä¼¼æ€§ï¼‰")
        print("  - ã‚°ãƒ©ãƒ•æ¤œç´¢: Neo4jï¼ˆæ§‹é€ çš„é–¢ä¿‚æ€§ï¼‰")
        print("  - çµ±åˆå›ç­”: LangChainã§ç”Ÿæˆï¼ˆLangSmithã§ã‚¦ã‚©ãƒƒãƒå¯èƒ½ï¼‰")
        print("=" * 50)
        question = input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()

        if not question:
            print("âŒ è³ªå•ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return False

        print(f"\nğŸ“ è³ªå•: {question}")
        print("ğŸ” ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ä¸­...")

        # è³ªå•ã‹ã‚‰é–¢æ•°åã‚‰ã—ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        vec_kw = question
        m_vec = re.search(
            r"`([^`]+)`|\"([^\"]+)\"|"
            r"([A-Za-z_][A-Za-z0-9_]*)",
            question,
        )
        if m_vec:
            vec_kw = next((g for g in m_vec.groups() if g), question)

        # 3. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢å®Ÿè¡Œï¼ˆLangChainã§ãƒ©ãƒƒãƒ—ï¼‰
        print("  â†’ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")
        vector_response = vector_search(vec_kw)

        print("  â†’ ã‚°ãƒ©ãƒ•æ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")
        # ã‚°ãƒ©ãƒ•æ¤œç´¢ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…·ä½“åŒ–ï¼ˆParameter/Type é–¢é€£ã‚’è¾¿ã‚‹ï¼‰
        graph_question = f"""
        Execute this Cypher to get a function and its parameters:
        MATCH (f:Function)
        WHERE toLower(f.name) CONTAINS toLower('{vec_kw}')
        OPTIONAL MATCH (p:Parameter)
        WHERE toLower(p.parent_function) = toLower(f.name)
        WITH f, collect(p) AS params
        RETURN f.name AS name,
               f.description AS description,
               [q IN params WHERE q IS NOT NULL AND q.name IS NOT NULL |
                {{name:q.name, description:q.description, required:coalesce(q.is_required,false)}}] AS parameters,
               null AS return_value
        LIMIT 5

        Then summarize the results in Japanese, focusing on:
        - Function name and description
        - Parameters (å¼•æ•°) with descriptions and whether required
        - Return value (æˆ»ã‚Šå€¤) if known; otherwise state ä¸æ˜
        """
        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ç©ºçµæœã‚’æ¤œçŸ¥ï¼ˆãƒ­ã‚°/ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç­‰ã«æ´»ç”¨å¯èƒ½ï¼‰
        def _on_empty(info):
            diag = info.get("diagnosis") if isinstance(info, dict) else None
            print("  â†’ ã‚°ãƒ©ãƒ•æ¤œç´¢çµæœãŒç©ºã§ã™ã€‚fallbackã‚’æ¤œè¨ã—ã¾ã™...")
            if diag:
                print(
                    "    è¨ºæ–­: "
                    f"Functionä»¶æ•°={diag.get('function_count')}, "
                    f"keywordä¸€è‡´ä»¶æ•°={diag.get('match_count_by_keyword')}, "
                    f"Parameterä»¶æ•°={diag.get('parameter_count')}"
                )
                names = diag.get("sample_function_names") or []
                if names:
                    print("    ã‚µãƒ³ãƒ—ãƒ«Functionå: " + ", ".join(map(str, names)))

        graph_response = graph_search(
            graph_question,
            on_empty=_on_empty,
            diagnose=True,
            keyword=vec_kw,
        )

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚°ãƒ©ãƒ•å¿œç­”ãŒç©ºã®å ´åˆã¯Neo4jã‚’ç›´æ¥æ¤œç´¢
        if not graph_response or str(graph_response).strip() in (
            "",
            "Empty Response",
        ):
            try:
                print("  â†’ ã‚°ãƒ©ãƒ•çµæœãŒç©ºã®ãŸã‚Neo4jã‚’ç›´æ¥ç…§ä¼š...")
                with GraphDatabase.driver(
                    config.neo4j_uri,
                    auth=(config.neo4j_user, config.neo4j_password),
                ) as driver:
                    with driver.session(
                        database=config.neo4j_database
                    ) as session:
                        cypher = (
                            "MATCH (f:Function) "
                            "WHERE toLower(f.name) CONTAINS toLower($kw) "
                            "OPTIONAL MATCH (p:Parameter) "
                            "WHERE toLower(p.parent_function) = toLower(f.name) "
                            "WITH f, collect(p) AS params "
                            "RETURN f.name AS name, f.description AS description, "
                            "[q IN params WHERE q.name IS NOT NULL | q] AS parameters, null AS return_value "
                            "LIMIT 5"
                        )
                        # ãƒ™ã‚¯ãƒˆãƒ«ç”¨ã«æŠ½å‡ºã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãã®ã¾ã¾ä½¿ç”¨
                        kw = vec_kw
                        rows = list(session.run(cypher, kw=kw))
                        if rows:
                            parts = []
                            for r in rows:
                                nm = r.get("name")
                                desc = r.get("description") or ""
                                params = r.get("parameters") or []
                                retv = r.get("return_value")

                                def _fmt_param(p):
                                    if isinstance(p, dict):
                                        n = p.get("name")
                                        d = p.get("description")
                                        req = p.get("is_required") or p.get("required")
                                        return f"- {n}: {d} (required={req})"
                                    n = getattr(p, "name", None)
                                    d = getattr(p, "description", None)
                                    req = getattr(p, "is_required", None)
                                    return f"- {n}: {d} (required={req})"

                                param_lines = []
                                try:
                                    for p in params:
                                        if p and (isinstance(p, dict) and p.get("name") or getattr(p, "name", None)):
                                            param_lines.append(_fmt_param(p))
                                except Exception:
                                    param_lines = []

                                section = [f"{nm}:", desc]
                                if param_lines:
                                    section.append("parameters:\n" + "\n".join(param_lines))
                                if retv:
                                    section.append(f"return_value: {retv}")
                                parts.append("\n".join(section))
                            graph_response = "\n\n".join(parts)
                        else:
                            graph_response = ""
            except Exception:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¤±æ•—æ™‚ã¯ãã®ã¾ã¾ç¶šè¡Œ
                pass

        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å›ç­”ã®çµ±åˆï¼ˆLangChainã§ç”Ÿæˆï¼‰
        print("  â†’ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å›ç­”ã‚’ç”Ÿæˆä¸­...")
        final_response = generate_response(vector_response, graph_response, question)

        # 4. çµæœã‚’è¡¨ç¤º
        print("\n" + "=" * 50)
        print("å›ç­”:")
        print("=" * 50)
        print(f"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœ: {vector_response}")
        print(f"ã‚°ãƒ©ãƒ•æ¤œç´¢çµæœ: {graph_response}")
        print(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢çµæœ: {final_response}")
        print("=" * 50)

        return True

    except Exception as e:
        print(f"QAã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="imabari3dcad ãƒ¡ã‚¤ãƒ³ - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè§£æã¨ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
        ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼:
        1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè§£æ â†’ Neo4jãƒ‡ãƒ¼ã‚¿æ ¼ç´
        2. ãƒ™ã‚¯ãƒˆãƒ«åŒ– â†’ ChromaDBæ§‹ç¯‰
        3. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ â†’ Neo4j + ChromaDB

        ä½¿ç”¨ä¾‹:
        python main_0905.py --function full_pipeline  # å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        python main_0905.py --function qa            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢
        python main_0905.py --function config        # è¨­å®šè¡¨ç¤º
        """
    )
    parser.add_argument("--function", "-f", help="å®Ÿè¡Œã™ã‚‹æ©Ÿèƒ½")
    parser.add_argument("--question", "-q", help="QAç”¨ã®è³ªå•ï¼ˆéå¯¾è©±ï¼‰")
    parser.add_argument("--list", "-l", action="store_true", help="æ©Ÿèƒ½ä¸€è¦§è¡¨ç¤º")
    # ã‚¯ãƒªã‚¢æ©Ÿèƒ½å‘ã‘è¿½åŠ å¼•æ•°
    parser.add_argument(
        "--db",
        dest="db",
        help=(
            "ã‚¯ãƒªã‚¢å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åï¼ˆæœªæŒ‡å®šæ™‚ã¯è¨­å®šã®NEO4J_DATABASEï¼‰"
        ),
    )
    parser.add_argument(
        "-y",
        "--yes",
        dest="yes",
        action="store_true",
        help="ç¢ºèªãªã—ã§å®Ÿè¡Œï¼ˆå±é™ºæ“ä½œã®ãŸã‚æ˜ç¤ºæŒ‡å®šãŒå¿…è¦ï¼‰",
    )
    args = parser.parse_args()
    # if args.list:
    #     print("åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½:")
    #     print("  code_generator  - AIã‚³ãƒ¼ãƒ‰ç”Ÿæˆ")
    #     print("  test_runner     - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    #     print("  doc_parser      - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè§£æ")
    #     print("  all            - å…¨æ©Ÿèƒ½å®Ÿè¡Œ")
    #     return

    print(f"å®Ÿè¡Œä¸­: {args.function}")
    config = Config()
    if args.function == "nollm_doc":
        success = run_nollm_doc(config)
    elif args.function == "llm_doc":
        success = run_llm_doc(config)
    elif args.function == "vectorize":
        success = run_vectorization(config)
    elif args.function == "full_pipeline":
        # å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: Neo4j â†’ ChromaDB â†’ LlamaIndex
        print("ğŸš€ å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œä¸­...")
        success = (
            run_llm_doc(config)
            and run_vectorization(config)
            and run_llamaindex_vectorization(config)
        )
    elif args.function == "qa":
        if args.question:
            # éå¯¾è©±ãƒ¢ãƒ¼ãƒ‰
            def _input(prompt: str = ""):
                return args.question or ""
            import builtins
            _orig_input = builtins.input
            try:
                builtins.input = _input  # type: ignore
                success = run_qa_system(config)
            finally:
                builtins.input = _orig_input  # type: ignore
        else:
            success = run_qa_system(config)
    elif args.function == "llamaindex_vectorize":
        success = run_llamaindex_vectorization(config)
    elif args.function == "clear_db":
        success = run_clear_database(
            config,
            database=args.db,
            force=args.yes,
        )
    elif args.function == "config":
        config.print_llm_config()
        success = True
    else:
        print(f"æœªçŸ¥ã®æ©Ÿèƒ½: {args.function}")
        success = False
    if success:
        print("âœ… å®Œäº†")
    else:
        print("âŒ ã‚¨ãƒ©ãƒ¼")
        sys.exit(1)


if __name__ == "__main__":
    main()
