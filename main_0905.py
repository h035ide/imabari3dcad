import sys
import argparse
import os
from pathlib import Path
from dotenv import load_dotenv

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()


class Config:
    """è¨­å®šã‚¯ãƒ©ã‚¹ - å…¨ã¦ã®è¨­å®šã‚’ä¸€å…ƒç®¡ç†"""

    def __init__(self):
        self.project_root = project_root

        # Neo4jè¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        self.neo4j_uri = os.getenv("NEO4J_URI", "neo4j://127.0.0.1:7687")
        self.neo4j_user = os.getenv("NEO4J_USER", "neo4j")
        self.neo4j_password = os.getenv("NEO4J_PASSWORD", "password")
        self.neo4j_database = os.getenv("NEO4J_DATABASE", "neo4j")

        # OpenAIè¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        self.openai_api_key = os.getenv("OPENAI_API_KEY")

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®š
        self.parsed_api_result_def_file = (
            "doc_parser/parsed_api_result_def.json"
        )
        self.parsed_api_result_file = "doc_parser/parsed_api_result.json"

        # Chromaè¨­å®š
        self.chroma_persist_directory = "chroma_db_store"
        self.chroma_collection_name = "api_documentation"

        # LlMè¨­å®š
        self.setup_llm_config()
        self.setup_embedding_config()
    
    def setup_llm_config(self):
        """LLMè¨­å®š"""
        # åŸºæœ¬è¨­å®š
        self.llm_model = "gpt-4o"
        self.response_format = "text"  # "json_object"
        # only for standard models
        self.llm_temperature = 0.1  # or None
        # only for inference models
        self.llm_verbosity = "high"  # "none" or "low" or "medium" or "high"
        self.llm_reasoning_effort = "high"  # "none" or "minimal" or "low" or "medium" or "high"
        
        # ãƒ¢ãƒ‡ãƒ«åˆ¤å®š
        self.is_inference_model = self._is_inference_model()
        
        # è¨­å®šè¾æ›¸ã‚’æ§‹ç¯‰
        self._build_llm_configs()
    
    def _is_inference_model(self):
        """æ¨è«–ãƒ¢ãƒ‡ãƒ«ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        inference_models = [
            "o4-mini", "o4", "gpt-5", "gpt-5-mini", "gpt-5-nano"
        ]
        return any(model in self.llm_model.lower()
                   for model in inference_models)
    
    def _build_llm_configs(self):
        """LLMè¨­å®šè¾æ›¸ã‚’æ§‹ç¯‰"""
        # åŸºæœ¬è¨­å®š
        base_config = {
            "api_key": self.openai_api_key
        }
        
        # LangChainç”¨è¨­å®š
        self.langchain_llm_config = {
            "model_name": self.llm_model,
            **base_config
        }
        
        # LlamaIndexç”¨è¨­å®š
        self.llamaindex_llm_config = {
            "model": self.llm_model,
            **base_config
        }
        
        # ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        if self.is_inference_model:
            self._add_inference_model_params()
        else:
            self._add_standard_model_params()
    
    def _add_inference_model_params(self):
        """æ¨è«–ãƒ¢ãƒ‡ãƒ«å°‚ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ """
        # æ¨è«–ãƒ¢ãƒ‡ãƒ«ã§ã¯temperatureã¯ä½¿ç”¨ã—ãªã„
        self.llm_temperature = None
        
        inference_params = {
            "reasoning_effort": self.llm_reasoning_effort,
            "output_version": "responses/v1",
            "verbosity": self.llm_verbosity,
            "response_format": self.response_format
        }
        
        for key, value in inference_params.items():
            self.langchain_llm_config[key] = value
            self.llamaindex_llm_config[key] = value
    
    def _add_standard_model_params(self):
        """é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ """
        # é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã§ã¯æ¨è«–ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä½¿ç”¨ã—ãªã„
        self.llm_verbosity = None
        self.llm_reasoning_effort = None
        
        # temperatureã‚’è¿½åŠ 
        if self.llm_temperature is not None:
            self.langchain_llm_config["temperature"] = self.llm_temperature
            self.llamaindex_llm_config["temperature"] = self.llm_temperature

    def setup_embedding_config(self):
        """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«è¨­å®š"""
        # åŸºæœ¬è¨­å®š
        self.embedding_model = "text-embedding-3-small"
        self.embedding_batch_size = 100
        
        # LangChainç”¨è¨­å®š
        self.langchain_embedding_config = {
            "model": self.embedding_model,
            "api_key": self.openai_api_key
        }
        
        # LlamaIndexç”¨è¨­å®š
        self.llamaindex_embedding_config = {
            "model": self.embedding_model,
            "batch_size": self.embedding_batch_size,
            "api_key": self.openai_api_key
        }
    
    def print_llm_config(self):
        """LLMè¨­å®šã‚’è¡¨ç¤º"""
        print("ğŸ¤– LLMè¨­å®š:")
        print(f"  ãƒ¢ãƒ‡ãƒ«: {self.llm_model}")
        print(f"  æ¨è«–ãƒ¢ãƒ‡ãƒ«: {'âœ…' if self.is_inference_model else 'âŒ'}")
        print(f"  Temperature: {self.llm_temperature}")
        print(f"  Response Format: {self.response_format}")
        
        if self.is_inference_model:
            print(f"  Verbosity: {self.llm_verbosity}")
            print(f"  Reasoning Effort: {self.llm_reasoning_effort}")
        
        print("\nğŸ“‹ LangChainè¨­å®š:")
        for key, value in self.langchain_llm_config.items():
            print(f"  {key}: {value}")
        print("\nğŸ“‹ LlamaIndexè¨­å®š:")
        for key, value in self.llamaindex_llm_config.items():
            print(f"  {key}: {value}")


def run_nollm_doc():
    """No LLM ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚’å®Ÿè¡Œ"""
    try:
        print("No LLM ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚’å®Ÿè¡Œä¸­...")
        # ã“ã“ã«å®Ÿéš›ã®å‡¦ç†ã‚’å®Ÿè£…
        return True
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def run_llm_doc():
    """LLM ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚’å®Ÿè¡Œ"""
    try:
        from doc_parser.neo4j_importer import import_to_neo4j

        # Configã‹ã‚‰è¨­å®šã‚’å–å¾—
        config = Config()

        # Neo4jã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        success = import_to_neo4j(
            uri=config.neo4j_uri,
            user=config.neo4j_user,
            password=config.neo4j_password,
            database=config.neo4j_database,
            file_path=config.parsed_api_result_def_file,
            use_def_file=True,
            config=config
        )

        return success
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def run_vectorization():
    """LlamaIndexã‚’ä½¿ç”¨ã—ãŸåŠ¹ç‡çš„ãªãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
    try:
        from code_generator.db.ingest_to_chroma import (
            fetch_data_from_neo4j,
            ingest_data_to_chroma
        )

        # Configã‹ã‚‰è¨­å®šã‚’å–å¾—
        config = Config()

        print("Neo4jã‹ã‚‰APIãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        # Functionãƒãƒ¼ãƒ‰ã‚’å–å¾—ï¼ˆNeo4jã®ãƒ©ãƒ™ãƒ«ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
        records = fetch_data_from_neo4j(
            label="Function",
            db_name=config.neo4j_database,
            allow_missing_description=True
        )

        if not records:
            print("ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return False

        print(f"{len(records)}ä»¶ã®APIé–¢æ•°ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­...")
        # Chromaã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ä¿å­˜
        ingest_data_to_chroma(
            records=records,
            collection_name=config.chroma_collection_name,
            persist_dir=config.chroma_persist_directory
        )

        print("âœ… Chromaãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†")
        return True

    except Exception as e:
        print(f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def run_llamaindex_vectorization():
    """LlamaIndexã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
    try:
        from llama_index.core import VectorStoreIndex, StorageContext, Settings
        from llama_index.vector_stores.chroma import ChromaVectorStore
        from llama_index.embeddings.openai import OpenAIEmbedding
        from llama_index.llms.openai import OpenAI
        import chromadb

        # Configã‹ã‚‰è¨­å®šã‚’å–å¾—
        config = Config()
        
        # è¨­å®šæƒ…å ±ã‚’è¡¨ç¤º
        print("ğŸ”§ LlamaIndexãƒ™ã‚¯ãƒˆãƒ«åŒ–è¨­å®š:")
        print(f"  LLMãƒ¢ãƒ‡ãƒ«: {config.llm_model}")
        print(f"  åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«: {config.embedding_model}")
        print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.embedding_batch_size}")
        print(f"  Chromaæ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {config.chroma_persist_directory}")
        print(f"  Chromaã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {config.chroma_collection_name}")
        
        if config.is_inference_model:
            print("  æ¨è«–ãƒ¢ãƒ‡ãƒ«è¨­å®š:")
            print(f"    Verbosity: {config.llm_verbosity}")
            print(f"    Reasoning Effort: {config.llm_reasoning_effort}")
        else:
            print("  æ¨™æº–ãƒ¢ãƒ‡ãƒ«è¨­å®š:")
            print(f"    Temperature: {config.llm_temperature}")

        # LlamaIndexã®è¨­å®š
        Settings.llm = OpenAI(**config.llamaindex_llm_config)
        Settings.embed_model = OpenAIEmbedding(**config.llamaindex_embedding_config)

        print("LlamaIndexã‚’ä½¿ç”¨ã—ãŸãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’é–‹å§‹...")

        # ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
        client = chromadb.PersistentClient(
            path=config.chroma_persist_directory
        )
        chroma_collection = client.get_or_create_collection(
            config.chroma_collection_name
        )

        # LlamaIndexã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        storage_context = StorageContext.from_defaults(
            vector_store=vector_store
        )

        # æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰
        VectorStoreIndex.from_vector_store(
            vector_store,
            storage_context=storage_context
        )

        print("âœ… LlamaIndexãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†")
        print(f"  â†’ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {config.chroma_collection_name}")
        print(f"  â†’ æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: "
              f"{config.chroma_persist_directory}")

        return True

    except Exception as e:
        print(f"LlamaIndexãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def run_qa_system():
    """LlamaIndexã‚’ä½¿ç”¨ã—ãŸåŠ¹ç‡çš„ãªQAã‚·ã‚¹ãƒ†ãƒ """
    try:
        from code_generator.llamaindex_integration import (
            build_vector_engine,
            build_graph_engine
        )
        from llama_index.core import Settings
        from llama_index.llms.openai import OpenAI

        # Configã‹ã‚‰è¨­å®šã‚’å–å¾—
        config = Config()


        # LlamaIndexã®è¨­å®š
        Settings.llm = OpenAI(**config.llamaindex_llm_config)

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è³ªå•ã‚’å…¥åŠ›ã—ã¦ã‚‚ã‚‰ã†
        print("\nğŸ” LlamaIndexçµ±åˆQAã‚·ã‚¹ãƒ†ãƒ ")
        print("=" * 50)
        print("ğŸ“‹ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ :")
        print("  â€¢ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢: ChromaDBï¼ˆæ„å‘³çš„é¡ä¼¼æ€§ï¼‰")
        print("  â€¢ ã‚°ãƒ©ãƒ•æ¤œç´¢: Neo4jï¼ˆæ§‹é€ çš„é–¢ä¿‚æ€§ï¼‰")
        print("  â€¢ çµ±åˆå›ç­”: ä¸¡æ–¹ã®çµæœã‚’çµ„ã¿åˆã‚ã›ãŸåŒ…æ‹¬çš„å›ç­”")
        print("=" * 50)
        question = input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()

        if not question:
            print("âŒ è³ªå•ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return False

        print(f"\nğŸ“ è³ªå•: {question}")
        print("ğŸ” ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ä¸­...")

        # 1. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’æ§‹ç¯‰
        print("  â†’ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’æ§‹ç¯‰ä¸­...")
        try:
            vector_engine = build_vector_engine(
                persist_dir=config.chroma_persist_directory,
                collection=config.chroma_collection_name
            )
        except Exception as e:
            print(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®æ§‹ç¯‰ã«å¤±æ•—: {e}")
            return False

        # 2. ã‚°ãƒ©ãƒ•æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’æ§‹ç¯‰
        print("  â†’ ã‚°ãƒ©ãƒ•æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’æ§‹ç¯‰ä¸­...")
        try:
            graph_engine = build_graph_engine()
        except Exception as e:
            print(f"âš ï¸ ã‚°ãƒ©ãƒ•æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®æ§‹ç¯‰ã«å¤±æ•—: {e}")
            print("  â†’ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿ã§å®Ÿè¡Œã—ã¾ã™...")
            graph_engine = None

        # 3. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢å®Ÿè¡Œ
        print("  â†’ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")
        vector_response = vector_engine.query(question)

        if graph_engine:
            print("  â†’ ã‚°ãƒ©ãƒ•æ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")
            graph_response = graph_engine.query(question)

            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å›ç­”ã®çµ±åˆ
            print("  â†’ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å›ç­”ã‚’ç”Ÿæˆä¸­...")
            combined_question = f"""
            ä»¥ä¸‹ã®2ã¤ã®æ¤œç´¢çµæœã‚’çµ±åˆã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«åŒ…æ‹¬çš„ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

            ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœã€‘
            {vector_response}

            ã€ã‚°ãƒ©ãƒ•æ¤œç´¢çµæœã€‘
            {graph_response}

            ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘
            {question}

            å›ç­”ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³:
            - ä¸¡æ–¹ã®æ¤œç´¢çµæœã®æƒ…å ±ã‚’çµ±åˆ
            - å…·ä½“çš„ãªAPIé–¢æ•°åã¨ãã®ä½¿ç”¨æ–¹æ³•ã‚’æ˜è¨˜
            - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è©³ç´°ã¨æˆ»ã‚Šå€¤ã«ã¤ã„ã¦èª¬æ˜
            - å®Ÿç”¨çš„ãªã‚³ãƒ¼ãƒ‰ä¾‹ãŒã‚ã‚Œã°æä¾›
            - ä¸æ˜ãªç‚¹ã¯æ­£ç›´ã«ã€Œä¸æ˜ã€ã¨å›ç­”
            """
            final_response = vector_engine.query(combined_question)
        else:
            final_response = vector_response

        # 4. çµæœã‚’è¡¨ç¤º
        print("\n" + "=" * 50)
        print("ğŸ¤– å›ç­”:")
        print("=" * 50)
        print(final_response)
        print("=" * 50)

        return True

    except Exception as e:
        print(f"âŒ QAã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="imabari3dcad ãƒ¡ã‚¤ãƒ³ - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè§£æã¨ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼:
1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè§£æ â†’ Neo4jãƒ‡ãƒ¼ã‚¿æ ¼ç´
2. ãƒ™ã‚¯ãƒˆãƒ«åŒ– â†’ ChromaDBæ§‹ç¯‰  
3. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ â†’ Neo4j + ChromaDB

ä½¿ç”¨ä¾‹:
  python main_0905.py --function full_pipeline  # å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
  python main_0905.py --function qa            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢
  python main_0905.py --function config        # è¨­å®šè¡¨ç¤º
        """
    )
    parser.add_argument("--function", "-f", help="å®Ÿè¡Œã™ã‚‹æ©Ÿèƒ½")
    parser.add_argument("--list", "-l", action="store_true", help="æ©Ÿèƒ½ä¸€è¦§è¡¨ç¤º")
    
    args = parser.parse_args()
    
    # if args.list:
    #     print("åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½:")
    #     print("  code_generator  - AIã‚³ãƒ¼ãƒ‰ç”Ÿæˆ")
    #     print("  test_runner     - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    #     print("  doc_parser      - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè§£æ")
    #     print("  all            - å…¨æ©Ÿèƒ½å®Ÿè¡Œ")
    #     return
    
    print(f"å®Ÿè¡Œä¸­: {args.function}")
    
    if args.function == "nollm_doc":
        success = run_nollm_doc()
    elif args.function == "llm_doc":
        success = run_llm_doc()
    elif args.function == "vectorize":
        success = run_vectorization()
    elif args.function == "llm_doc_and_vectorize":
        success = (run_llm_doc() and run_vectorization())
    elif args.function == "full_pipeline":
        # å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: Neo4j â†’ ChromaDB â†’ LlamaIndex
        print("ğŸš€ å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œä¸­...")
        success = (run_llm_doc() and 
                  run_vectorization() and 
                  run_llamaindex_vectorization())
    elif args.function == "qa":
        success = run_qa_system()
    elif args.function == "llamaindex_vectorize":
        success = run_llamaindex_vectorization()
    elif args.function == "config":
        config = Config()
        config.print_llm_config()
        success = True
    else:
        print(f"æœªçŸ¥ã®æ©Ÿèƒ½: {args.function}")
        success = False
    
    if success:
        print("âœ… å®Œäº†")
    else:
        print("âŒ ã‚¨ãƒ©ãƒ¼")
        sys.exit(1)


if __name__ == "__main__":
    main()
