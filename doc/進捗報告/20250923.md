# LangChainとLangGraphによるRAG・AIエージェント[実践]入門
## 4章 - RAGパイプラインの構築

### Document loader
- **目的**: 様々な形式のドキュメントを読み込み、LangChainで処理可能な形式に変換
- **主要なloader**:
  - `TextLoader`: テキストファイルの読み込み
  - `PyPDFLoader`: PDFファイルの読み込み
  - `DirectoryLoader`: ディレクトリ内の複数ファイルを一括読み込み
  - `WebBaseLoader`: Webページの読み込み
  - `UnstructuredFileLoader`: Unstructuredライブラリを使用した高精度なドキュメント解析
- **実装例**:
  ```python
  from langchain.document_loaders import PyPDFLoader
  loader = PyPDFLoader("document.pdf")
  documents = loader.load()
  ```
- **Unstructured loaderの特徴**:
  - **高精度な構造解析**: 複雑なレイアウトのドキュメントでも正確にテキストを抽出
  - **多様なファイル形式対応**: PDF、Word、PowerPoint、HTML、Markdown等
  - **構造保持**: 見出し、リスト、表などの構造情報を保持
  - **メタデータ抽出**: 著者、作成日、タイトルなどの情報を自動抽出
  - **実装例**:
    ```python
    from langchain.document_loaders import UnstructuredFileLoader
    loader = UnstructuredFileLoader("document.pdf")
    documents = loader.load()
    
    # 特定の要素タイプのみ抽出
    from langchain.document_loaders import UnstructuredFileLoader
    loader = UnstructuredFileLoader(
        "document.pdf",
        mode="elements",
        strategy="fast"  # または "hi_res" で高精度
    )
    ```

### Document transformer
- **目的**: 読み込んだドキュメントを適切なサイズに分割し、メタデータを追加
- **主要な機能**:
  - `RecursiveCharacterTextSplitter`: 文字数ベースでドキュメントを分割
  - `TokenTextSplitter`: トークン数ベースで分割
  - `Html2TextTransformer`: HTMLドキュメントをテキストに変換
  - チャンクサイズとオーバーラップの調整
- **実装例**:
  ```python
  from langchain.text_splitter import RecursiveCharacterTextSplitter
  
  # RecursiveCharacterTextSplitterの使用
  text_splitter = RecursiveCharacterTextSplitter(
      chunk_size=1000,
      chunk_overlap=200,
      separators=["\n\n", "\n", " ", ""]  # 階層的な区切り文字
  )
  texts = text_splitter.split_documents(documents)
  ```

### Embedding model
- **目的**: テキストをベクトル表現に変換し、意味的な類似性を計算可能にする
- **主要なモデル**:
  - `OpenAIEmbeddings`: OpenAIのtext-embedding-ada-002
  - `HuggingFaceEmbeddings`: HuggingFaceの埋め込みモデル
  - `SentenceTransformerEmbeddings`: sentence-transformersライブラリ
- **実装例**:
  ```python
  from langchain.embeddings import OpenAIEmbeddings
  embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
  ```

### Vector store
- **目的**: 埋め込みベクトルを効率的に保存・検索するためのデータベース
- **主要なストア**:
  - `Chroma`: 軽量で高速なベクトルDB
- **実装例**:
  ```python
  from langchain.vectorstores import Chroma
  vectorstore = Chroma.from_documents(
      documents=texts,
      embedding=embeddings
  )
  ```

## 6章 - 高度なRAG技術

### HyDE(Hypothetical Document Embeddings)
- **Hypothetical Document Embedder**:
  - **目的**: クエリに対する仮想的な回答文書を生成し、その埋め込みベクトルで検索を実行
  - **仕組み**: 
    1. ユーザークエリに対してLLMが仮想的な回答を生成
    2. その回答の埋め込みベクトルを作成
    3. そのベクトルでドキュメントを検索
  - **利点**: クエリと実際の回答文書の意味的類似性を高める
  - **実装例**:
    ```python
    from langchain.retrievers import HyDE
    from langchain.embeddings import OpenAIEmbeddings
    from langchain.llms import OpenAI
    
    embeddings = OpenAIEmbeddings()
    llm = OpenAI()
    hyde_retriever = HyDE.from_llm(llm, embeddings, vectorstore)
    ```

### 複数の検索クエリの生成
- **MultiQueryRetriever**:
  - **目的**: 1つのクエリから複数の異なるクエリを生成し、多角的な検索を実行
  - **仕組み**:
    1. 元のクエリからLLMが複数のバリエーションクエリを生成
    2. 各クエリで検索を実行
    3. 結果を統合して重複を除去
  - **利点**: 検索の網羅性と精度の向上
  - **実装例**:
    ```python
    from langchain.retrievers.multi_query import MultiQueryRetriever
    
    retriever = MultiQueryRetriever.from_llm(
        retriever=vectorstore.as_retriever(),
        llm=llm
    )
    docs = retriever.get_relevant_documents("検索クエリ")
    ```

### RAG-Fusion
- **目的**: 複数の検索戦略を組み合わせて最適な結果を取得
- **構成要素**:
  - **Vector Search**: 意味的類似性検索
  - **Keyword Search**: キーワードマッチング
  - **Hybrid Search**: 両方の結果を統合
- **実装例**:
  ```python
  from langchain.retrievers import EnsembleRetriever
  from langchain.retrievers import BM25Retriever
  
  # ベクトル検索とBM25検索を組み合わせ
  vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
  bm25_retriever = BM25Retriever.from_texts(texts)
  ensemble_retriever = EnsembleRetriever(
      retrievers=[vector_retriever, bm25_retriever],
      weights=[0.7, 0.3]
  )
  ```

### リランクモデル
- **目的**: 初期検索結果を再ランキングして精度を向上
- **主要なモデル**:
  - **Cohere Rerank**: Cohere社のリランクモデル
  - **Cross-Encoder**: 双方向エンコーダーによる再評価
  - **Custom Reranker**: 独自のリランクロジック
- **実装例**:
  ```python
  from langchain.retrievers import ContextualCompressionRetriever
  from langchain.retrievers.document_compressors import CohereRerank
  
  compressor = CohereRerank(top_n=3)
  compression_retriever = ContextualCompressionRetriever(
      base_compressor=compressor,
      base_retriever=vectorstore.as_retriever()
  )
  ```

### ハイブリッド検索
- **EnsembleRetriever**:
  - **目的**: 複数の検索手法の結果を統合して最適な検索を実現
  - **主要な組み合わせ**:
    - ベクトル検索 + BM25検索
    - セマンティック検索 + キーワード検索
    - 異なる埋め込みモデルの組み合わせ
  - **重み付け**: 各検索手法の重要度を調整可能
  - **実装例**:
    ```python
    from langchain.retrievers import EnsembleRetriever
    from langchain.retrievers import BM25Retriever
    
    # 複数の検索器を組み合わせ
    retrievers = [vector_retriever, bm25_retriever, keyword_retriever]
    weights = [0.5, 0.3, 0.2]
    
    ensemble_retriever = EnsembleRetriever(
        retrievers=retrievers,
        weights=weights
    )
    ```

# HTMLファイルの処理手法



<!-- ここから追記: doc_preprocessor_hybrid 概要と処理フロー -->

## doc_preprocessor_hybrid の概要
- **目的**: EVO.SHIP APIドキュメント（`api.txt`/`api_arg.txt`）から、決定的なルールベース抽出＋必要箇所のみLLM補強を行い、構造化JSON・グラフ・ベクターチャンクを生成し、任意でNeo4j/Chromaへ保存。
- **主要モジュール**:
  - `rule_parser.py`: ルールベース抽出（型定義・API仕様の正規化、ソース断片の追跡）
  - `llm_enricher.py`: LLM補強（不足説明の補完、返り値型の改善、差分適用）
  - `graph_builder.py`: グラフ用ノード/関係の構築（Object/Method/Type/Parameter）
  - `pipeline.py`: オーケストレーション（既存成果物の再利用、出力生成、ストレージ連携）
  - `schemas.py`: データスキーマ（`ApiBundle`/`ApiEntry`/`TypeDefinition`等）
  - `cli.py`: CLIエントリポイント

### 技術的ポイント
- **決定的抽出の強化**: 型の正規化（日本語→`canonical_type`/`py_type`）、2D/3D変種の自動展開、必須判定、配列・次元の抽出、ソース断片ハッシュで監査可能性を確保。
- **LLM補強の限定化**: 変更はJSON差分のみ適用し、原文抜粋（近傍行）と型定義コンテキストを提示。`OPENAI_API_KEY`未設定時はスキップし安全動作。
- **出力物**: `structured_api.json`/`structured_api_enriched.json`、`graph_payload.json`、`vector_chunks.jsonl`。
- **外部連携**: Neo4j（`BELONGS_TO`/`RETURNS`/`HAS_PARAMETER`/`HAS_TYPE`）とChroma（要約済みチャンク）を任意保存。

### 処理フロー（draw.io）
- 図版: `doc/doc_preprocessor_hybrid_flow.drawio`
- 概略: `cli.py` → `pipeline.run_pipeline()` → ルールベース抽出 → （任意）LLM補強 → グラフ構築/ベクターチャンク生成 → 成果物出力 → （任意）Neo4j/Chroma保存。

### 追記: 実行手順（uv）
- 依存インストール: `uv pip install -r requirements.txt`
- ルールベースのみ: `uv run python -m doc_preprocessor_hybrid.cli --api-doc data/src/api.txt --api-arg data/src/api_arg.txt --output-dir doc_preprocessor_hybrid/out`
- LLM補強あり: `uv run python -m doc_preprocessor_hybrid.cli --api-doc data/src/api.txt --api-arg data/src/api_arg.txt --output-dir doc_preprocessor_hybrid/out --llm`
- Neo4j保存: `uv run python -m doc_preprocessor_hybrid.cli --store-neo4j`
- Chroma保存: `uv run python -m doc_preprocessor_hybrid.cli --store-chroma`


# ドキュメント検索

RAG では、問い合わせに応えるためのコンテキスト候補をここで収集する。検索器は再現率と精度のトレードオフがあるため、複数方式を組み合わせて評価ログで継続改善する。

### 要約（クイックガイド）

- 目的: 「語一致」と「意味理解」を両立し、必要に応じて関係（グラフ）で根拠を補強する
- 標準フロー:
  1. メタデータ前処理（権限・期間・ドメイン）
  2. BM25 k=50 と 密ベクトル k=50 を並列検索 → RRF/重み付きで統合
  3. 軽量整形（要約/重要文抽出）
  4. リランクを上位30件に適用→最終5件
  5. 構造的問いは Neo4j で補強（必要時）
  6. 空振り時のみ MultiQuery/HyDE で再取得
- プロファイル例:
  - 低遅延: BM25 20 + Dense 20 → RRF → 生成（リランクOFF）
  - 標準: BM25 50 + Dense 50 → リランク30→5
  - 高精度: 標準 + MultiQuery/HyDE（不足時）+ グラフ照会
- 選択の指針:
  - 抽象/曖昧: Dense 重み↑、必要に応じ MultiQuery/HyDE
  - 固有名詞/品番: BM25 重み↑、k↑（SPLADE 追加も検討）
  - 関係・依存・型/引数: 先に Neo4j で構造検索→テキスト補完
- 実装参照: 下記の「実装例（LangChain/LlamaIndex）」「統合例（LangChain × LlamaIndex）」「Neo4j を用いたドキュメント検索（詳細）」「適用順と条件分岐（推奨運用）」

## 全文検索（BM25 系）
**概要**: 語単位の倒立インデックスを使い、BM25 や TF-IDF のスコアでランキングする語彙ベース検索。固有名詞や品番を取りこぼさない。

- 主な構成要素
  - トークナイズ/形態素解析（MeCab、Janome、Sudachi）で語境界と原形を確定
  - 倒立インデックス `語 -> (文書ID, 出現位置, 回数)` の構築
  - BM25 パラメータ `k1,b` の調整（文書長補正と語頻度飽和の制御）
- 長所: 高速・軽量、説明しやすい、レア語に強い
- 短所: 言い換え/文脈理解が弱い、スペルの揺れに弱い
- プロジェクトでの扱い: `data/whoosh_index/` を活かして BM25 検索を提供し、ハイブリッド検索の片翼とする

### Whoosh による実装（推奨）

Whoosh は純 Python 製の全文検索ライブラリで、BM25F を含むスコアリング、柔軟なスキーマ、クエリパーサ、ハイライト、ファセットなどを備える。公式ドキュメントを参照のこと（設計・実装の要点は下記に要約）: [Whoosh 2.7.4 documentation](https://whoosh.readthedocs.io/en/latest/index.html)。

#### スキーマ設計（例）

- フィールドの役割を分離する。
  - `doc_id`: 一意キー（更新/再取り込みで使用）
  - `title`: タイトル（語ベース検索）
  - `content`: 本文（語ベース検索、ハイライト用に `stored=True` 推奨）
  - `content_ngram`: 日本語や表記揺れ対策のための文字 N-gram（2〜4gram）
  - `created_at`: 日付ソート/フィルタ用

```python
from whoosh import fields
from whoosh import index
from whoosh import analysis
from pathlib import Path

schema = fields.Schema(
    doc_id=fields.ID(stored=True, unique=True),
    title=fields.TEXT(stored=True,
                      analyzer=analysis.RegexTokenizer() | analysis.LowercaseFilter()),
    content=fields.TEXT(stored=True,
                        analyzer=analysis.RegexTokenizer() | analysis.LowercaseFilter()),
    # 日本語の簡易対応: 文字N-gram（MeCab等が無い環境でも動作）
    content_ngram=fields.NGRAM(minsize=2, maxsize=4, stored=False),
    created_at=fields.DATETIME(stored=True),
)

idx_dir = Path("data/whoosh_index")
idx_dir.mkdir(parents=True, exist_ok=True)
ix = index.create_in(str(idx_dir), schema) if not index.exists_in(str(idx_dir)) else index.open_dir(str(idx_dir))
```

ポイント
- 日本語の高精度対応が必要なら、形態素解析で `content` を語ベースにし、`content_ngram` をフォールバック用に併用。
- スキーマ変更後は再構築が原則（過去セグメントとの互換は限定的）。

#### インデックス作成/更新（バッチ）

```python
from datetime import datetime

# Windows では procs>0 は環境によりオーバーヘッドが大きい場合がある
writer = ix.writer(limitmb=256, procs=0, multisegment=True)

for doc in iterable_docs:  # ユーザー実装: タイトル/本文/作成日を供給
    writer.update_document(
        doc_id=doc["id"],
        title=doc["title"],
        content=doc["content"],
        content_ngram=doc["content"],  # N-gram は生テキストを渡す
        created_at=datetime.fromisoformat(doc["created_at"]) if isinstance(doc["created_at"], str) else doc["created_at"],
    )

# 大量取り込み時は multisegment=True でコミットし、巡回メンテナンスでマージ
writer.commit(merge=True)
```

Tips（速度最適化）
- `limitmb`: インメモリバッファ上限。大きくするとセグメントが減りコミットが高速化。
- `procs`: マルチプロセスでの解析/索引化（Linux 環境で効果が出やすい）。
- `multisegment`: True にするとコミット時の完全マージを避けて高速化（後でマージ）。

参考: 「Tips for speeding up batch indexing」, 「How to index documents」

#### 検索・ハイライト・並べ替え

```python
from whoosh.qparser import MultifieldParser, OrGroup
from whoosh import scoring
from whoosh import highlight
from whoosh.sorting import FieldFacet, MultiFacet

query_text = "半径 AND 穴"  # 例
with ix.searcher(weighting=scoring.BM25F()) as searcher:
    parser = MultifieldParser(["title", "content", "content_ngram"], schema=ix.schema, group=OrGroup)
    q = parser.parse(query_text)

    # 例: 日付降順→スコアで安定ソート
    facets = MultiFacet([FieldFacet("created_at", reverse=True)])
    results = searcher.search(q, limit=50, sortedby=facets, terms=True)

    # ハイライト設定（周辺文脈を短めに）
    results.fragmenter = highlight.ContextFragmenter(maxchars=180, surround=60)
    results.formatter = highlight.HtmlFormatter(tagname="mark")

    top5 = []
    for hit in results[:5]:
        top5.append({
            "doc_id": hit["doc_id"],
            "title": hit.get("title", ""),
            "snippet": hit.highlights("content", text=hit.get("content", "")),
            "score": hit.score,
        })
```

ポイント
- `BM25F()` はフィールド横断の BM25（F）重み付け。必要に応じて `scoring.TF_IDF()` へ切替可能。
- `MultifieldParser` に `content_ngram` を含めると表記揺れに強くなる（同義語は別途）。
- 結果には `terms=True` を付けると一致語の把握が容易（デバッグに有用）。

参考: 「How to search」, 「How to create highlighted search result excerpts」, 「Sorting and faceting」

#### 「Did you mean…?」候補提示（任意）

```python
with ix.searcher() as s:
    corr = s.corrector("content")
    suggestions = corr.suggest("raius", limit=3)  # 綴り誤りの補正候補
```

参考: 「“Did you mean... ?” Correcting errors in user queries」

#### タイムリミット/安全策（任意）

大量ヒット時や重クエリでは、時間制限を設けて応答性を守る。

```python
from whoosh.collectors import TimeLimitCollector, SimpleCollector, TimeLimit

with ix.searcher(weighting=scoring.BM25F()) as s:
    q = MultifieldParser(["title", "content"], schema=ix.schema).parse(query_text)
    base = SimpleCollector()
    tlc = TimeLimitCollector(base, timelimit=0.8)  # 秒
    try:
        s.search_with_collector(q, tlc)
        results = tlc.results()
    except TimeLimit:
        results = tlc.results()  # 途中までの結果を使用
```

参考: 「Time limited searches」

#### ロック/運用（重要）

- Whoosh は排他制御のため書き込み時に `MAIN_WRITELOCK` を作成する。異常終了で残ると追加入力がブロックされる。
  - 他プロセスからの書き込みが確実に無いことを確認のうえ、残置ロックを削除して再実行（`data/whoosh_index/MAIN_WRITELOCK`）。
  - 通常は `writer.commit()` か `writer.cancel()` がロックを解放する。
- 読み込みは複数同時でも安全。書き込みは単一ライター原則。

参考: 「Concurrency, locking, and versioning」

#### 日本語対応の実務指針

- まずは `content` を形態素解析（MeCab/Janome/Sudachi）で語ベースに、`content_ngram` に文字 N-gram を併用。
- 品番・品名・記号や半角/全角の揺れは `NGRAM(2–4)` が有効。必要に応じ正規化（全半角/大文字小文字/記号除去）。
- 「検索の網羅性↑」が目的なら `content_ngram` の重みを高める（RRF で BM25×ベクトル 検索に統合）。

#### RAG への統合

- 上位 `k` 件の `doc_id/title/snippet` を LLM に提示。根拠性のため `doc_id` を回答に含める。
- ベクトル検索と組み合わせる場合は、BM25 と Dense の結果を RRF で融合 → リランク。

---

参考資料: [Whoosh 2.7.4 documentation](https://whoosh.readthedocs.io/en/latest/index.html)

## ベクトル検索（セマンティック検索）
**概要**: クエリと文書を埋め込みモデルで連続ベクトル化し、類似度（コサイン・内積など）の近さで取得する。意味的な言い換えに対応できる。

- 主な構成要素
  - 埋め込みモデル（`OpenAIEmbeddings`、`bge`、E5 など）でチャンクをベクトル化
  - 近似最近傍探索（ANN: HNSW、IVF-PQ、ScaNN）で大規模コーパスを低遅延検索
  - 結果本文は RAG プロンプトに入れる前にサマリ/重要文抽出で整形
- 長所: 同義語・多言語・長文に強い、概念検索に向く
- 短所: モデル依存、誤ヒットが説明しづらい、インデックスがメモリ重め
- プロジェクトでの扱い: 標準は `Chroma` + `OpenAIEmbeddings`／`HuggingFaceEmbeddings`。推論コストと精度で埋め込みモデルを選定

### 密ベクトル（Dense Embedding）
- Transformer 系モデルで 384〜3072 次元程度の密なベクトルを生成
- 類似度はコサイン/内積が基本。トークン分解不要で文脈意味を保持
- HNSW や IVF-PQ、DiskANN 等で ANN 化し、再構築コストに注意
- 代表例: `text-embedding-3-small`, `bge-large-ja`, `sentence-transformers/all-mpnet-base-v2`

### 疎ベクトル（Sparse Embedding）
- 語彙次元の高次元ベクトルでほぼゼロ。TF-IDF、BM25、SPLADE などで生成
- 倒立インデックスで検索可能。密ベクトルより軽く、語一致に強い
- 同義語には弱い。SPLADE などの学習疎表現は語展開機能を持つ
- 密ベクトルと RRF（Reciprocal Rank Fusion）や加重和で統合すると精度と再現率を両立

## ハイブリッド＆リランク
- **ハイブリッド検索**: 密ベクトルと疎ベクトル/BM25 を統合し、語一致と意味対応を両立。`EnsembleRetriever` や RRF で実装
- **リランク**: 上位 `k` 件に Cross-Encoder や Cohere Rerank を適用し、文脈一致度を再ランキング（コストとのトレードオフ）
- **クエリ拡張**: MultiQueryRetriever や HyDE でクエリを多様化し、取りこぼしを減らす

## グラフDB（Graph-RAG）
**概要**: API/型/パラメータなどの知識をノードとエッジで格納し、Cypher でパターン探索。検索結果の根拠提示と依存関係追跡を支援する。

- ノード例: `Object`、`Method`、`Type`、`Parameter`
- 関係例: `BELONGS_TO`、`RETURNS`、`HAS_PARAMETER`、`HAS_TYPE`
- 使用例
  ```cypher
  MATCH (m:Method)-[:HAS_PARAMETER]->(p:Parameter {name: "radius"})
  RETURN m.name, m.id
  LIMIT 5;
  ```
- プロジェクトでの扱い: `doc_preprocessor_hybrid` が `graph_payload.json` を生成し、`db_integration` 経由で Neo4j へロード。RAG プロンプトにノード属性を付与、もしくは検索結果の根拠として提示

### Neo4j を用いたドキュメント検索（詳細）

- 基本方針
  - 「どのドキュメント（チャンク）が、どのエンティティ（Object/Method/Type/Parameter）に関連するか」をグラフで表現
  - テキストは外部ベクトルDB（Chroma 等）に置き、Neo4j は構造的ナビゲーションと根拠付けを担当

- 代表的な Cypher パターン
  - メソッドとその引数を持つチャンク候補
    ```cypher
    MATCH (m:Method)-[:HAS_PARAMETER]->(p:Parameter)
    MATCH (c:Chunk)-[:DESCRIBES]->(m)
    WHERE p.name = $param_name
    RETURN c.id AS chunk_id, m.name AS method, p.name AS param
    LIMIT 50;
    ```
  - 型に紐づく API 一覧
    ```cypher
    MATCH (t:Type)<-[:HAS_TYPE]-(p:Parameter)<-[:HAS_PARAMETER]-(m:Method)
    RETURN t.name AS type, collect(distinct m.name) AS methods
    LIMIT 50;
    ```
  - オブジェクト配下のメソッドを階層で探索
    ```cypher
    MATCH (o:Object {name: $object})-[:BELONGS_TO*0..2]->(ns)
    MATCH (m:Method)-[:BELONGS_TO]->(ns)
    OPTIONAL MATCH (m)-[:HAS_PARAMETER]->(p:Parameter)
    RETURN m.name AS method, collect(p.name) AS params
    ORDER BY method
    LIMIT 100;
    ```

- LangChain 連携の流れ
  1. Neo4j で候補メソッド/型/パラメータを取得（Cypher）
  2. 候補 ID/名称をクエリ拡張語に変換（例: メソッド名＋主要引数名）
  3. ベクトル検索（Chroma）で該当チャンクを取得→LCEL で生成

- LlamaIndex 連携の流れ
  1. Neo4j で対象エンティティ集合を取得
  2. エンティティに紐づくチャンクIDをキーに、LlamaIndex の `VectorStoreIndex` でサブセット検索
  3. 取得ノードをプロンプトへ（出典: chunk_id, method, param）

- 設計メモ
  - グラフは「関係の索引」、ベクトルDBは「本文の索引」。役割を分離
  - `Chunk` ノードは粒度を一定に（例: 512〜1,024 tokens）し、`DESCRIBES`/`MENTIONS` で関連付け
  - 監査性のために `source_path`, `start_line`, `end_line`, `hash` を `Chunk` に保持

### 実装例（LangChain）

以下は最小構成の例。`OPENAI_API_KEY` など必要な環境変数を設定して利用する。

- 全文検索（BM25）
```python
from langchain_community.retrievers import BM25Retriever

texts = ["doc1 ...", "doc2 ..."]  # 分割済みチャンク
bm25 = BM25Retriever.from_texts(texts, k=5)
docs = bm25.invoke("検索クエリ")
```

- ベクトル検索（Chroma + OpenAI Embeddings）
```python
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

emb = OpenAIEmbeddings(model="text-embedding-3-small")
vs = Chroma(collection_name="docs", embedding_function=emb)
vs.add_texts(texts)
vec = vs.as_retriever(search_kwargs={"k": 5})
docs = vec.invoke("検索クエリ")
```

- ハイブリッド（BM25 × ベクトル）
```python
from langchain.retrievers import EnsembleRetriever

hybrid = EnsembleRetriever(retrievers=[bm25, vec], weights=[0.4, 0.6])
docs = hybrid.invoke("検索クエリ")
```

- クエリ拡張（MultiQuery）
```python
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
multi = MultiQueryRetriever.from_llm(retriever=vec, llm=llm)
docs = multi.get_relevant_documents("検索クエリ")
```

- Graph-RAG（Neo4j）
```python
import os
from langchain_neo4j import Neo4jGraph

graph = Neo4jGraph(
    url=os.environ["NEO4J_URI"],
    username=os.environ["NEO4J_USER"],
    password=os.environ["NEO4J_PASSWORD"],
)
records = graph.query(
    """
MATCH (m:Method)-[:HAS_PARAMETER]->(p:Parameter {name: "radius"})
RETURN m.name, m.id LIMIT 5
    """
)
```

### 実装例（LlamaIndex）

- ベクトル検索（Chroma 連携）
```python
from llama_index.core import VectorStoreIndex, Settings, Document
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

texts = ["doc1 ...", "doc2 ..."]
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

client = chromadb.PersistentClient(path="chroma")
col = client.get_or_create_collection("docs")
vstore = ChromaVectorStore(chroma_collection=col)

docs = [Document(text=t) for t in texts]
index = VectorStoreIndex.from_documents(docs, vector_store=vstore)
retriever = index.as_retriever(similarity_top_k=5)
nodes = retriever.retrieve("検索クエリ")
```

- 全文検索（BM25）
```python
from llama_index.core import Document
from llama_index.core.retrievers import BM25Retriever

docs = [Document(text=t) for t in texts]
bm25 = BM25Retriever.from_documents(docs, k=5)
nodes = bm25.retrieve("検索クエリ")
```

- ハイブリッド（Query Fusion + RRF）
```python
from llama_index.core.retrievers import QueryFusionRetriever

fusion = QueryFusionRetriever(
    retrievers=[bm25, index.as_retriever(similarity_top_k=5)],
    similarity_top_k=5,
    num_queries=4,
    mode="reciprocal_rerank",  # RRF
)
nodes = fusion.retrieve("検索クエリ")
```

- リランク（Cross-Encoder）
```python
from llama_index.core.postprocessor import SentenceTransformerRerank

rerank = SentenceTransformerRerank(
    model="cross-encoder/ms-marco-MiniLM-L-6-v2", top_n=5
)
nodes = rerank.postprocess_nodes(nodes, query_str="検索クエリ")
```

- Graph-RAG（Neo4j 連携の最小）
```python
import os
from neo4j import GraphDatabase

driver = GraphDatabase.driver(
    os.environ["NEO4J_URI"],
    auth=(os.environ["NEO4J_USER"], os.environ["NEO4J_PASSWORD"]),
)
with driver.session() as s:
    res = s.run(
        """
        MATCH (m:Method)-[:HAS_PARAMETER]->(p:Parameter {name: "radius"})
        RETURN m.name, m.id LIMIT 5
        """
    ).data()
```

### 統合例（LangChain × LlamaIndex）

- LlamaIndex のリトリーバを LangChain にブリッジしてハイブリッド＋生成

```python
# LlamaIndex 側: Chroma 上にインデックスを構築
from llama_index.core import VectorStoreIndex, Settings, Document
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core.bridge.langchain import as_langchain_retriever
import chromadb

texts = ["doc1 ...", "doc2 ..."]
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

client = chromadb.PersistentClient(path="chroma")
col = client.get_or_create_collection("docs_mix")
vstore = ChromaVectorStore(chroma_collection=col)
li_docs = [Document(text=t) for t in texts]
index = VectorStoreIndex.from_documents(li_docs, vector_store=vstore)

# LangChain 側: BM25 と LlamaIndex リトリーバをアンサンブル
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever

bm25 = BM25Retriever.from_texts(texts, k=5)
li_as_lc = as_langchain_retriever(index.as_retriever(similarity_top_k=5))
hybrid = EnsembleRetriever(retrievers=[bm25, li_as_lc], weights=[0.4, 0.6])

# LCEL で生成まで（最小例）
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
prompt = ChatPromptTemplate.from_template(
    """
質問: {q}
コンテキスト:\n{context}
日本語で簡潔に回答してください。
"""
)

def format_docs(docs):
    return "\n\n".join(d.page_content for d in docs)

chain = ({"context": hybrid | format_docs, "q": RunnablePassthrough()}
         | prompt
         | llm
         | StrOutputParser())

answer = chain.invoke("検索クエリ")
```

- LangChain の BM25 で絞り込み → LlamaIndex のベクトルで最終化

```python
# LC: まず語一致で候補を増やす
from langchain_community.retrievers import BM25Retriever
bm25 = BM25Retriever.from_texts(texts, k=20)
lexical_docs = bm25.invoke("検索クエリ")

# LI: LC ドキュメントから再インデックスして意味検索（少数に絞って高速）
from llama_index.core import VectorStoreIndex, Document
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import Settings

Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")
li_subset = [Document(text=d.page_content, metadata=d.metadata) for d in lexical_docs]
sub_index = VectorStoreIndex.from_documents(li_subset)
final_nodes = sub_index.as_retriever(similarity_top_k=5).retrieve("検索クエリ")

# 最終的に LlamaIndex ノードから本文を抽出
final_context = "\n\n".join([n.node.get_content() for n in final_nodes])
```

### 適用順と条件分岐（推奨運用）

- 標準フロー
  1. メタデータ前処理（権限・期間・ドメインで事前フィルタ）
  2. ハイブリッド初期取得（並列）: BM25 k=50 + 密ベクトル k=50 → RRF/重み付き統合
  3. 軽量整形（要約/重要文抽出）でトークン最適化
  4. リランク: Cross-Encoder を上位30件に適用→最終5件
  5. グラフ補強（必要時）: Neo4j で関係・依存を照会し根拠/属性を付与
  6. フォールバック（不足時）: MultiQuery/HyDE を発火して再取得
  7. 生成: 出典・スコアを付けて LLM へ投入

- 条件分岐
  - 抽象/曖昧な質問: 密ベクトルの重み↑、MultiQuery/HyDE を許可
  - 固有名詞・品番が多い: BM25 の重み↑、k↑、（必要に応じ SPLADE）
  - 構造照会（〜を持つ関数/型）: 先にグラフ→テキスト補完
  - 厳しいSLA: リランクOFF or 件数縮小、フォールバックは失敗時のみ

- 運用プロファイル例
  - 低遅延: BM25 k=20 + Dense k=20 → RRF → 生成（リランクOFF）
  - 高精度: BM25 k=50 + Dense k=50 → リランク30→5 → 不足時 MultiQuery/HyDE


## graph_engine.query(query) の整理（LlamaIndex PropertyGraphQueryEngine）

- **実装位置**: `main_helper_0905.py` の `build_graph_engine(config)` が `Neo4jPropertyGraphStore` と `PropertyGraphIndex.from_existing(...)` を初期化し、`index.as_query_engine(llm=...)` を返す。その戻り値の `query(query_str)` を実行。

- **処理フロー**: 自然文 →（LLMがスキーマを踏まえた）Cypher 自動生成 → Neo4j 実行 → 要約生成（文字列化可能なレスポンス）。LlamaIndex の標準 `as_query_engine().query(...)` パターン。

- **依存/前提**:
  - **Neo4j**: `NEO4J_URI`/`NEO4J_USER`/`NEO4J_PASSWORD`/`NEO4J_DATABASE`（APOC プラグイン前提）。
  - **LLM/Embedding**: OpenAI（`OPENAI_API_KEY`）。`config.llamaindex_llm_config` / `config.llamaindex_embedding_config` を利用。
  - **DB名の一貫性**: 接続系は `config.neo4j_database` に統一（DB名不一致は失敗要因）。

- **返り値と空検知**: `graph_search_wrapper()` では `str(response)` を正規化し、`""`/`"Empty Response"` 等で空を判定。診断モードでは件数・キーワード一致数・サンプル名などを Neo4j 直接クエリで取得。

- **既知の注意点（重要）**:
  - `graph_engine.query(...)` は「自然文→Cypher 自動生成→DB 実行→要約」の黒箱。プロンプト内に Cypher を直接書いても、そのまま実行される保証はない（多くの場合“指示文”として扱われる）。
  - 本プロジェクトのグラフは `Function` ノード直下に `parameters/return_value` プロパティを持たず、`Parameter`/`Type` ノードと `HAS_PARAMETER`/`HAS_TYPE`/`RETURNS` リレーションで表現。LLM がこのスキーマに沿った Cypher を生成できないと空振りしやすい。

- **安定化の選択肢**:
  - **明示 Cypher 実行**（推奨）: Neo4j ドライバでクエリ文字列を直接発行（`GraphDatabase.driver(...).session().run(cypher, params)`）。確実に実行され、再現性が高い。
  - **テンプレート駆動**: LlamaIndex の `CypherTemplateRetriever` で Cypher テンプレート＋型付きパラメータから実行（自然文生成の不確実性を低減）。
  - （環境によっては）**Cypher専用エンジン**が提供されるバージョンもあるため、利用可能なら切替を検討。

- **プロンプト設計のヒント**（自然文→Cypher 自動生成の成功率を上げる）:
  - 対象ラベル/プロパティ/関係（例: `Function-[:HAS_PARAMETER]->Parameter-[:HAS_TYPE]->Type`, `[:RETURNS]`）を明示。
  - 出力形式を厳密に指定（例: JSON で `function`, `parameters`, `types` のみ、説明文なし）。
  - 曖昧表現を避け、簡潔な英語で要件を書く（多言語より英語が安定しやすい）。

- **実行例**:
  - CLI（QA 確認）: `uv run python main_0905.py -f qa -q "<question>"`
  - 直接利用（概念図）:

    ```python
    from main_helper_0905 import Config, build_graph_engine

    config = Config()
    graph_engine = build_graph_engine(config)
    resp = graph_engine.query("List functions related to radius; return JSON only.")
    print(str(resp))
    ```

- **参考**（LlamaIndex docs）:
  - プロパティグラフ × Neo4j の最小例: `index.as_query_engine(...).query("...")`
  - スキーマ取得/テンプレート: `CypherTemplateRetriever`（クエリを型付きで安全に発行）
  - 公式の概念/API 参照は LlamaIndex Property Graph/Neo4j セクション（安定版ドキュメント）

- **補足（本リポジトリの実装要点）**:
  - `build_graph_engine(config)` は `OpenAI` LLM と `OpenAIEmbedding` を明示指定し、`PropertyGraphIndex.from_existing(...)` → `as_query_engine(llm=...)` で構築。
  - デバッグ用途で `Function` サンプルや `db.schema.nodeTypeProperties()` を取得する補助ロジックあり（APOC 必須）。
  - グラフ検索結果はベクトル検索結果と統合して最終回答を生成する設計（LangChain ラッパー経由）。
