# LangChainとLangGraphによるRAG・AIエージェント[実践]入門
## 4章 - RAGパイプラインの構築

### Document loader
- **目的**: 様々な形式のドキュメントを読み込み、LangChainで処理可能な形式に変換
- **主要なloader**:
  - `TextLoader`: テキストファイルの読み込み
  - `PyPDFLoader`: PDFファイルの読み込み
  - `DirectoryLoader`: ディレクトリ内の複数ファイルを一括読み込み
  - `WebBaseLoader`: Webページの読み込み
  - `UnstructuredFileLoader`: Unstructuredライブラリを使用した高精度なドキュメント解析
- **実装例**:
  ```python
  from langchain.document_loaders import PyPDFLoader
  loader = PyPDFLoader("document.pdf")
  documents = loader.load()
  ```
- **Unstructured loaderの特徴**:
  - **高精度な構造解析**: 複雑なレイアウトのドキュメントでも正確にテキストを抽出
  - **多様なファイル形式対応**: PDF、Word、PowerPoint、HTML、Markdown等
  - **構造保持**: 見出し、リスト、表などの構造情報を保持
  - **メタデータ抽出**: 著者、作成日、タイトルなどの情報を自動抽出
  - **実装例**:
    ```python
    from langchain.document_loaders import UnstructuredFileLoader
    loader = UnstructuredFileLoader("document.pdf")
    documents = loader.load()
    
    # 特定の要素タイプのみ抽出
    from langchain.document_loaders import UnstructuredFileLoader
    loader = UnstructuredFileLoader(
        "document.pdf",
        mode="elements",
        strategy="fast"  # または "hi_res" で高精度
    )
    ```

### Document transformer
- **目的**: 読み込んだドキュメントを適切なサイズに分割し、メタデータを追加
- **主要な機能**:
  - `RecursiveCharacterTextSplitter`: 文字数ベースでドキュメントを分割
  - `TokenTextSplitter`: トークン数ベースで分割
  - `Html2TextTransformer`: HTMLドキュメントをテキストに変換
  - チャンクサイズとオーバーラップの調整
- **実装例**:
  ```python
  from langchain.text_splitter import RecursiveCharacterTextSplitter
  
  # RecursiveCharacterTextSplitterの使用
  text_splitter = RecursiveCharacterTextSplitter(
      chunk_size=1000,
      chunk_overlap=200,
      separators=["\n\n", "\n", " ", ""]  # 階層的な区切り文字
  )
  texts = text_splitter.split_documents(documents)
  ```

### Embedding model
- **目的**: テキストをベクトル表現に変換し、意味的な類似性を計算可能にする
- **主要なモデル**:
  - `OpenAIEmbeddings`: OpenAIのtext-embedding-ada-002
  - `HuggingFaceEmbeddings`: HuggingFaceの埋め込みモデル
  - `SentenceTransformerEmbeddings`: sentence-transformersライブラリ
- **実装例**:
  ```python
  from langchain.embeddings import OpenAIEmbeddings
  embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
  ```

### Vector store
- **目的**: 埋め込みベクトルを効率的に保存・検索するためのデータベース
- **主要なストア**:
  - `Chroma`: 軽量で高速なベクトルDB
- **実装例**:
  ```python
  from langchain.vectorstores import Chroma
  vectorstore = Chroma.from_documents(
      documents=texts,
      embedding=embeddings
  )
  ```

## 6章 - 高度なRAG技術

### HyDE(Hypothetical Document Embeddings)
- **Hypothetical Document Embedder**:
  - **目的**: クエリに対する仮想的な回答文書を生成し、その埋め込みベクトルで検索を実行
  - **仕組み**: 
    1. ユーザークエリに対してLLMが仮想的な回答を生成
    2. その回答の埋め込みベクトルを作成
    3. そのベクトルでドキュメントを検索
  - **利点**: クエリと実際の回答文書の意味的類似性を高める
  - **実装例**:
    ```python
    from langchain.retrievers import HyDE
    from langchain.embeddings import OpenAIEmbeddings
    from langchain.llms import OpenAI
    
    embeddings = OpenAIEmbeddings()
    llm = OpenAI()
    hyde_retriever = HyDE.from_llm(llm, embeddings, vectorstore)
    ```

### 複数の検索クエリの生成
- **MultiQueryRetriever**:
  - **目的**: 1つのクエリから複数の異なるクエリを生成し、多角的な検索を実行
  - **仕組み**:
    1. 元のクエリからLLMが複数のバリエーションクエリを生成
    2. 各クエリで検索を実行
    3. 結果を統合して重複を除去
  - **利点**: 検索の網羅性と精度の向上
  - **実装例**:
    ```python
    from langchain.retrievers.multi_query import MultiQueryRetriever
    
    retriever = MultiQueryRetriever.from_llm(
        retriever=vectorstore.as_retriever(),
        llm=llm
    )
    docs = retriever.get_relevant_documents("検索クエリ")
    ```

### RAG-Fusion
- **目的**: 複数の検索戦略を組み合わせて最適な結果を取得
- **構成要素**:
  - **Vector Search**: 意味的類似性検索
  - **Keyword Search**: キーワードマッチング
  - **Hybrid Search**: 両方の結果を統合
- **実装例**:
  ```python
  from langchain.retrievers import EnsembleRetriever
  from langchain.retrievers import BM25Retriever
  
  # ベクトル検索とBM25検索を組み合わせ
  vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
  bm25_retriever = BM25Retriever.from_texts(texts)
  ensemble_retriever = EnsembleRetriever(
      retrievers=[vector_retriever, bm25_retriever],
      weights=[0.7, 0.3]
  )
  ```

### リランクモデル
- **目的**: 初期検索結果を再ランキングして精度を向上
- **主要なモデル**:
  - **Cohere Rerank**: Cohere社のリランクモデル
  - **Cross-Encoder**: 双方向エンコーダーによる再評価
  - **Custom Reranker**: 独自のリランクロジック
- **実装例**:
  ```python
  from langchain.retrievers import ContextualCompressionRetriever
  from langchain.retrievers.document_compressors import CohereRerank
  
  compressor = CohereRerank(top_n=3)
  compression_retriever = ContextualCompressionRetriever(
      base_compressor=compressor,
      base_retriever=vectorstore.as_retriever()
  )
  ```

### ハイブリッド検索
- **EnsembleRetriever**:
  - **目的**: 複数の検索手法の結果を統合して最適な検索を実現
  - **主要な組み合わせ**:
    - ベクトル検索 + BM25検索
    - セマンティック検索 + キーワード検索
    - 異なる埋め込みモデルの組み合わせ
  - **重み付け**: 各検索手法の重要度を調整可能
  - **実装例**:
    ```python
    from langchain.retrievers import EnsembleRetriever
    from langchain.retrievers import BM25Retriever
    
    # 複数の検索器を組み合わせ
    retrievers = [vector_retriever, bm25_retriever, keyword_retriever]
    weights = [0.5, 0.3, 0.2]
    
    ensemble_retriever = EnsembleRetriever(
        retrievers=retrievers,
        weights=weights
    )
    ```

# HTMLファイルの処理手法



<!-- ここから追記: doc_preprocessor_hybrid 概要と処理フロー -->

## doc_preprocessor_hybrid の概要
- **目的**: EVO.SHIP APIドキュメント（`api.txt`/`api_arg.txt`）から、決定的なルールベース抽出＋必要箇所のみLLM補強を行い、構造化JSON・グラフ・ベクターチャンクを生成し、任意でNeo4j/Chromaへ保存。
- **主要モジュール**:
  - `rule_parser.py`: ルールベース抽出（型定義・API仕様の正規化、ソース断片の追跡）
  - `llm_enricher.py`: LLM補強（不足説明の補完、返り値型の改善、差分適用）
  - `graph_builder.py`: グラフ用ノード/関係の構築（Object/Method/Type/Parameter）
  - `pipeline.py`: オーケストレーション（既存成果物の再利用、出力生成、ストレージ連携）
  - `schemas.py`: データスキーマ（`ApiBundle`/`ApiEntry`/`TypeDefinition`等）
  - `cli.py`: CLIエントリポイント

### 技術的ポイント
- **決定的抽出の強化**: 型の正規化（日本語→`canonical_type`/`py_type`）、2D/3D変種の自動展開、必須判定、配列・次元の抽出、ソース断片ハッシュで監査可能性を確保。
- **LLM補強の限定化**: 変更はJSON差分のみ適用し、原文抜粋（近傍行）と型定義コンテキストを提示。`OPENAI_API_KEY`未設定時はスキップし安全動作。
- **出力物**: `structured_api.json`/`structured_api_enriched.json`、`graph_payload.json`、`vector_chunks.jsonl`。
- **外部連携**: Neo4j（`BELONGS_TO`/`RETURNS`/`HAS_PARAMETER`/`HAS_TYPE`）とChroma（要約済みチャンク）を任意保存。

### 処理フロー（draw.io）
- 図版: `doc/doc_preprocessor_hybrid_flow.drawio`
- 概略: `cli.py` → `pipeline.run_pipeline()` → ルールベース抽出 → （任意）LLM補強 → グラフ構築/ベクターチャンク生成 → 成果物出力 → （任意）Neo4j/Chroma保存。