ステップ1: API仕様書の解析
_read_api_text() と _read_api_arg_text():
 
APIのメソッド仕様が書かれた api.txt と、データ型の説明が書かれた api_arg.txt を読み込みます。
 
_normalize_text():
 
読み込んだテキストからBOM（Byte Order Mark）や余分な空白、改行コードの違いなどを正規化し、後の処理がしやすいように整形します。
 
_parse_data_type_descriptions():
 
api_arg.txt を解析し、「文字列: 通常の文字列」のような、データ型名とその説明を対応付けた辞書データを作成します。
 
extract_triples_from_specs():
 
api.txt の内容を本格的に解析し、グラフの元となる「トリプル」（ノード、関係、ノードの3つ組）を生成します。
 
内部で _parse_api_specs() を呼び出し、正規表現を駆使してテキストからオブジェクト名、メソッド名、説明、引数、返り値などを抽出します。
 
抽出した情報から、「CreatePlate メソッドは Part オブジェクトに 属する (BELONGS_TO)」「CreatePlate メソッドは pname という引数を 持つ (HAS_PARAMETER)」といった関係性を定義します。
 
ステップ2: スクリプト例の解析
_read_test_script():
 
APIの使用例が書かれた test.py を読み込みます。
 
extract_triples_from_script():
 
スクリプトのテキストから、API呼び出しに関するトリプルを生成します。
 
この関数の核となるのが _extract_method_calls_from_script() です。
 
tree-sitter という高度なコードパーサーライブラリを使い、Pythonコードを単なる文字列としてではなく、構文木（AST）として解析します。
 
object.method() のようなメソッド呼び出しの箇所を正確に特定し、オブジェクト名やメソッド名、引数などの情報を抽出します。
 
抽出した呼び出し情報から、「test.py スクリプトは MethodCall_0 という呼び出しを 含む (CONTAINS)」「MethodCall_0 は CreatePlate メソッドを 呼び出す (CALLS)」「MethodCall_0 の 次に (NEXT) MethodCall_1 が実行される」といった関係性を定義します。
 
ステップ3: データの統合とNeo4jへの格納
_triples_to_graph_documents():
 
ステップ1と2で生成したトリプルとノード情報をLangChainが扱える GraphDocument 形式に変換します。
 
_rebuild_graph_in_neo4j():
 
Neo4jデータベースに接続します。
 
まず既存のデータをすべて削除 (MATCH (n) DETACH DELETE n) し、データベースを初期化します。
 
その後、生成した GraphDocument を使って新しいグラフデータを一括で投入します。
 
4. Chromaベクトルデータベースの構築 (_build_and_load_chroma_from_specs)
この処理は、APIの仕様書 (api.txt) の情報のみを利用します。
 
_parse_api_specs():
 
Neo4jの構築時と同様に、api.txt を解析して、各APIメソッドの情報を構造化されたデータとして抽出します。
 
_build_and_load_chroma_from_specs():
 
抽出したAPIメソッド情報（オブジェクト名、メソッド名、説明、引数など）を、人間が読みやすい形式のテキスト（Document）に変換します。
 
既存のChromaデータベースがあれば削除し、ディレクトリを再作成します。
 
OpenAIEmbeddings を使い、各 Document の内容を数値のベクトル（埋め込み表現）に変換します。
 
変換したベクトルデータを Chroma データベースに保存（永続化）します。これにより、あとで「〇〇をするためのAPI」といった自然言語のクエリに対して、意味的に類似したAPIドキュメントを高速に検索できるようになります。