import sys
import argparse
import os
from pathlib import Path
from dotenv import load_dotenv

class Config:
    """è¨­å®šã‚¯ãƒ©ã‚¹ - å…¨ã¦ã®è¨­å®šã‚’ä¸€å…ƒç®¡ç†"""

    def __init__(self):
        self.project_root = Path(__file__).parent

        # Neo4jè¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        self.neo4j_uri = os.getenv("NEO4J_URI", "neo4j://127.0.0.1:7687")
        self.neo4j_user = os.getenv("NEO4J_USER", "neo4j")
        self.neo4j_password = os.getenv("NEO4J_PASSWORD", "password")
        self.neo4j_database = os.getenv("NEO4J_DATABASE", "neo4j")

        # OpenAIè¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        self.openai_api_key = os.getenv("OPENAI_API_KEY")

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®š
        self.parsed_api_result_def_file = (
            "doc_parser/parsed_api_result_def.json"
        )
        self.parsed_api_result_file = "doc_parser/parsed_api_result.json"

        # Chromaè¨­å®š
        self.chroma_persist_directory = "chroma_db_store"
        self.chroma_collection_name = "api_documentation"

        # LlMè¨­å®š
        self.setup_llm_config()
        self.setup_embedding_config()
    
    def setup_llm_config(self):
        """LLMè¨­å®š"""
        # åŸºæœ¬è¨­å®š
        self.llm_model = "gpt-4o"
        self.response_format = "text"  # "json_object"
        # only for standard models
        self.llm_temperature = 0.1  # or None
        # only for inference models
        self.llm_verbosity = "high"  # "none" or "low" or "medium" or "high"
        self.llm_reasoning_effort = "high"  # "none" or "minimal" or "low" or "medium" or "high"
        
        # ãƒ¢ãƒ‡ãƒ«åˆ¤å®š
        self.is_inference_model = self._is_inference_model()
        
        # è¨­å®šè¾æ›¸ã‚’æ§‹ç¯‰
        self._build_llm_configs()
    
    def _is_inference_model(self):
        """æ¨è«–ãƒ¢ãƒ‡ãƒ«ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        inference_models = [
            "o4-mini", "o4", "gpt-5", "gpt-5-mini", "gpt-5-nano"
        ]
        return any(model in self.llm_model.lower()
                   for model in inference_models)
    
    def _build_llm_configs(self):
        """LLMè¨­å®šè¾æ›¸ã‚’æ§‹ç¯‰"""
        # åŸºæœ¬è¨­å®š
        base_config = {
            "api_key": self.openai_api_key
        }
        
        # LangChainç”¨è¨­å®š
        self.langchain_llm_config = {
            "model_name": self.llm_model,
            **base_config
        }
        
        # LlamaIndexç”¨è¨­å®š
        self.llamaindex_llm_config = {
            "model": self.llm_model,
            **base_config
        }
        
        # ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        if self.is_inference_model:
            self._add_inference_model_params()
        else:
            self._add_standard_model_params()
    
    def _add_inference_model_params(self):
        """æ¨è«–ãƒ¢ãƒ‡ãƒ«å°‚ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ """
        # æ¨è«–ãƒ¢ãƒ‡ãƒ«ã§ã¯temperatureã¯ä½¿ç”¨ã—ãªã„
        self.llm_temperature = None
        
        inference_params = {
            "reasoning_effort": self.llm_reasoning_effort,
            "output_version": "responses/v1",
            "verbosity": self.llm_verbosity,
            "response_format": self.response_format
        }
        
        for key, value in inference_params.items():
            self.langchain_llm_config[key] = value
            self.llamaindex_llm_config[key] = value
    
    def _add_standard_model_params(self):
        """é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ """
        # é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã§ã¯æ¨è«–ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä½¿ç”¨ã—ãªã„
        self.llm_verbosity = None
        self.llm_reasoning_effort = None
        
        # temperatureã‚’è¿½åŠ 
        if self.llm_temperature is not None:
            self.langchain_llm_config["temperature"] = self.llm_temperature
            self.llamaindex_llm_config["temperature"] = self.llm_temperature

    def setup_embedding_config(self):
        """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«è¨­å®š"""
        # åŸºæœ¬è¨­å®š
        self.embedding_model = "text-embedding-3-small"
        self.embedding_batch_size = 100
        
        # LangChainç”¨è¨­å®š
        self.langchain_embedding_config = {
            "model": self.embedding_model,
            "api_key": self.openai_api_key
        }
        
        # LlamaIndexç”¨è¨­å®š
        self.llamaindex_embedding_config = {
            "model": self.embedding_model,
            "batch_size": self.embedding_batch_size,
            "api_key": self.openai_api_key
        }
    
    def print_llm_config(self):
        """LLMè¨­å®šã‚’è¡¨ç¤º"""
        print("ğŸ¤– LLMè¨­å®š:")
        print(f"  ãƒ¢ãƒ‡ãƒ«: {self.llm_model}")
        print(f"  æ¨è«–ãƒ¢ãƒ‡ãƒ«: {'âœ…' if self.is_inference_model else 'âŒ'}")
        print(f"  Temperature: {self.llm_temperature}")
        print(f"  Response Format: {self.response_format}")
        
        if self.is_inference_model:
            print(f"  Verbosity: {self.llm_verbosity}")
            print(f"  Reasoning Effort: {self.llm_reasoning_effort}")
        
        print("\nğŸ“‹ LangChainè¨­å®š:")
        for key, value in self.langchain_llm_config.items():
            print(f"  {key}: {value}")
        print("\nğŸ“‹ LlamaIndexè¨­å®š:")
        for key, value in self.llamaindex_llm_config.items():
            print(f"  {key}: {value}")
