import os
from pathlib import Path
import logging
from typing import Optional
from neo4j import GraphDatabase
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
import chromadb

# LlamaIndex imports
from llama_index.core import (
    Settings,
    VectorStoreIndex,
    StorageContext,
)
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore
from llama_index.core.indices.property_graph import PropertyGraphIndex

logger = logging.getLogger(__name__)


class Config:
    """è¨­å®šã‚¯ãƒ©ã‚¹ - å…¨ã¦ã®è¨­å®šã‚’ä¸€å…ƒç®¡ç†"""

    def __init__(self):
        self.project_root = Path(__file__).parent

        # Neo4jè¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        self.neo4j_uri = self._normalize_neo4j_uri(
            os.getenv("NEO4J_URI", "bolt://127.0.0.1:7687")
        )
        self.neo4j_user = os.getenv("NEO4J_USER", "neo4j")
        self.neo4j_password = os.getenv("NEO4J_PASSWORD", "password")
        self.neo4j_database = os.getenv("NEO4J_DATABASE", "neo4j")

        # OpenAIè¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        self.openai_api_key = os.getenv("OPENAI_API_KEY")

        # APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¨­å®šï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆåŸºæº–ã®çµ¶å¯¾ãƒ‘ã‚¹ã€‚ç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯ï¼‰
        default_api_dir = self.project_root / "data" / "src"
        self.api_document_dir = Path(os.getenv("API_DOCUMENT_DIR", str(default_api_dir)))

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®š
        self.parsed_api_result_def_file = "doc_parser/parsed_api_result_def.json"
        self.parsed_api_result_file = "doc_parser/parsed_api_result.json"

        # Chromaè¨­å®š
        self.chroma_persist_directory = "chroma_db_store"
        self.chroma_collection_name = "api_documentation"

        # LlMè¨­å®š
        self.setup_llm_config()
        self.setup_embedding_config()

    def _normalize_neo4j_uri(self, uri: str) -> str:
        """ãƒ­ãƒ¼ã‚«ãƒ«å˜ä¸€ãƒãƒ¼ãƒ‰æ¥ç¶šã§ã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚URIã‚’æ­£è¦åŒ–ã€‚
        - neo4j://localhost(127.0.0.1) â†’ bolt://localhost(127.0.0.1)
        - ãƒãƒ¼ãƒˆæœªæŒ‡å®šæ™‚ã¯ :7687 ã‚’ä»˜ä¸
        """
        try:
            u = uri.strip()
            if u.startswith("neo4j://") and ("localhost" in u or "127.0.0.1" in u):
                u = "bolt://" + u[len("neo4j://"):]
            if u.startswith("bolt://"):
                host_port = u[len("bolt://"):]
                # ãƒãƒ¼ãƒˆãŒç„¡ã‘ã‚Œã°7687ã‚’ä»˜ä¸
                if ":" not in host_port:
                    u = u + ":7687"
            return u
        except Exception:
            # å¤±æ•—ã—ã¦ã‚‚å…ƒã®å€¤ã‚’è¿”ã™
            return uri

    def setup_llm_config(self):
        """LLMè¨­å®š"""
        # åŸºæœ¬è¨­å®š
        self.llm_model = "gpt-5-mini"
        self.response_format = "text"  # "json_object"
        # only for standard models
        self.llm_temperature = 0  # or None
        # only for inference models
        self.llm_verbosity = "high"  # "none" or "low" or "medium" or "high"
        # "none" or "minimal" or "low" or "medium" or "high"
        self.llm_reasoning_effort = "high"

        # ãƒ¢ãƒ‡ãƒ«åˆ¤å®š
        self.is_inference_model = self._is_inference_model()

        # è¨­å®šè¾æ›¸ã‚’æ§‹ç¯‰
        self._build_llm_configs()

    def _is_inference_model(self):
        """æ¨è«–ãƒ¢ãƒ‡ãƒ«ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        inference_models = ["o4-mini", "o4", "gpt-5", "gpt-5-mini", "gpt-5-nano"]
        return any(model in self.llm_model.lower() for model in inference_models)

    def _build_llm_configs(self):
        """LLMè¨­å®šè¾æ›¸ã‚’æ§‹ç¯‰"""
        # åŸºæœ¬è¨­å®š
        base_config = {"api_key": self.openai_api_key}

        # LangChainç”¨è¨­å®š
        self.langchain_llm_config = {"model_name": self.llm_model, **base_config}

        # LlamaIndexç”¨è¨­å®š
        self.llamaindex_llm_config = {"model": self.llm_model, **base_config}

        # ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        if self.is_inference_model:
            self._add_inference_model_params()
        else:
            self._add_standard_model_params()

    def _add_inference_model_params(self):
        """æ¨è«–ãƒ¢ãƒ‡ãƒ«å°‚ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ """
        # æ¨è«–ãƒ¢ãƒ‡ãƒ«ã§ã¯temperatureã¯ä½¿ç”¨ã—ãªã„
        self.llm_temperature = None

        inference_params = {
            "reasoning_effort": self.llm_reasoning_effort,
            "output_version": "responses/v1",
            "verbosity": self.llm_verbosity,
            "response_format": self.response_format,
        }

        for key, value in inference_params.items():
            self.langchain_llm_config[key] = value
            self.llamaindex_llm_config[key] = value

    def _add_standard_model_params(self):
        """é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ """
        # é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã§ã¯æ¨è«–ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä½¿ç”¨ã—ãªã„
        self.llm_verbosity = None
        self.llm_reasoning_effort = None

        # temperatureã‚’è¿½åŠ 
        if self.llm_temperature is not None:
            self.langchain_llm_config["temperature"] = self.llm_temperature
            self.llamaindex_llm_config["temperature"] = self.llm_temperature

    def setup_embedding_config(self):
        """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«è¨­å®š"""
        # åŸºæœ¬è¨­å®š
        self.embedding_model = "text-embedding-3-small"
        self.embedding_batch_size = 100

        # LangChainç”¨è¨­å®š
        self.langchain_embedding_config = {
            "model": self.embedding_model,
            "api_key": self.openai_api_key,
        }

        # LlamaIndexç”¨è¨­å®š
        self.llamaindex_embedding_config = {
            "model": self.embedding_model,
            "batch_size": self.embedding_batch_size,
            "api_key": self.openai_api_key,
        }

    def print_llm_config(self):
        """LLMè¨­å®šã‚’è¡¨ç¤º"""
        print("ğŸ¤– LLMè¨­å®š:")
        print(f"  ãƒ¢ãƒ‡ãƒ«: {self.llm_model}")
        print(f"  æ¨è«–ãƒ¢ãƒ‡ãƒ«: {'âœ…' if self.is_inference_model else 'âŒ'}")
        print(f"  Temperature: {self.llm_temperature}")
        print(f"  Response Format: {self.response_format}")

        if self.is_inference_model:
            print(f"  Verbosity: {self.llm_verbosity}")
            print(f"  Reasoning Effort: {self.llm_reasoning_effort}")

        print("\nğŸ“‹ LangChainè¨­å®š:")
        for key, value in self.langchain_llm_config.items():
            print(f"  {key}: {value}")
        print("\nğŸ“‹ LlamaIndexè¨­å®š:")
        for key, value in self.llamaindex_llm_config.items():
            print(f"  {key}: {value}")


def fetch_data_from_neo4j(
    label: str = "ApiFunction",
    db_name: Optional[str] = None,
    allow_missing_description: bool = True,
    config: Optional[Config] = None,
):
    """
    Neo4jã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™ã€‚
    label: å–å¾—å¯¾è±¡ã®ãƒ©ãƒ™ãƒ«ï¼ˆä¾‹: ApiFunction, Functionï¼‰
    db_name: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åï¼ˆæœªæŒ‡å®šãªã‚‰ç’°å¢ƒå¤‰æ•°ã®NEO4J_DATABASEã¾ãŸã¯codeparsarï¼‰
    allow_missing_description: descriptionæ¬ å¦‚æ™‚ã‚‚å–å¾—ã™ã‚‹ã‹
    """
    if config is None:
        logger.error("ConfigãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return []

    logger.info(f"Neo4jãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ ({config.neo4j_uri}) ã«æ¥ç¶šã—ã¦ã„ã¾ã™...")
    try:
        with GraphDatabase.driver(
            config.neo4j_uri, auth=(config.neo4j_user, config.neo4j_password)
        ) as driver:
            database = db_name or os.getenv("NEO4J_DATABASE", "codeparsar")
            with driver.session(database=database) as session:
                if allow_missing_description:
                    query = f"""
                    MATCH (n:{label})
                    WHERE n.name IS NOT NULL
                    RETURN elementId(n) AS node_id, n.name AS name,
                           n.description AS description
                    """
                else:
                    query = f"""
                    MATCH (n:{label})
                    WHERE n.name IS NOT NULL AND n.description IS NOT NULL
                    RETURN elementId(n) AS node_id, n.name AS name,
                           n.description AS description
                    """
                logger.info(f"{label} ãƒãƒ¼ãƒ‰ã‚’å–å¾—ã—ã¦ã„ã¾ã™ï¼ˆdatabase={database}ï¼‰...")
                result = session.run(query)  # type: ignore
                records = list(result)
                logger.info(f"{len(records)}ä»¶ã®{label}ãƒãƒ¼ãƒ‰ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
                return records
    except Exception as e:
        logger.error(
            f"Neo4jã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True
        )
        return []


def ingest_data_to_chroma(
    records,
    collection_name: Optional[str] = None,
    persist_dir: Optional[str] = None,
    config: Optional[Config] = None,
):
    """å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ChromaDBã«æ ¼ç´ã—ã¾ã™ã€‚"""
    if not records:
        logger.warning("æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
    if collection_name is None:
        collection_name = (
            config.chroma_collection_name if config else "api_documentation"
        )
    if persist_dir is None:
        persist_dir = config.chroma_persist_directory if config else "chroma_db_store"
    if config is None:
        logger.error("ConfigãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    documents = []
    metadatas = []
    ids = []

    for record in records:
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€æ¤œç´¢å¯¾è±¡ã¨ãªã‚‹ãƒ†ã‚­ã‚¹ãƒˆã€‚
        # åå‰ã¨èª¬æ˜ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€æ¤œç´¢ç²¾åº¦å‘ä¸Šã‚’ç‹™ã†ã€‚
        description = record.get("description") or ""
        doc_content = f"APIå: {record['name']}\nèª¬æ˜: {description}"
        documents.append(doc_content)

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ã¯ã€å¾Œã§ã‚°ãƒ©ãƒ•ã‚’å†æ¤œç´¢ã™ã‚‹ãŸã‚ã«å¿…è¦ãªæƒ…å ±ã‚’æ ¼ç´
        metadatas.append(
            {"api_name": record["name"], "neo4j_node_id": record["node_id"]}
        )

        # ChromaDBå†…ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªIDã¨ã—ã¦ã€Neo4jã®ãƒãƒ¼ãƒ‰IDã‚’ä½¿ç”¨
        ids.append(record["node_id"])

    logger.info(f"{len(documents)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ChromaDBã«æ ¼ç´ã—ã¾ã™...")
    logger.info(f"ChromaDBæ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {persist_dir}")
    logger.info(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å: {collection_name}")

    try:
        # OpenAIã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
        api_key = config.openai_api_key
        if api_key is None:
            logger.error("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return
        # LangChainå´ã®åŸ‹ã‚è¾¼ã¿ã‚‚ config ã®ãƒ¢ãƒ‡ãƒ«ã«çµ±ä¸€
        embedding_function = OpenAIEmbeddings(
            **config.langchain_embedding_config
        )  # type: ignore

        # ChromaDBã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ï¼ˆã¾ãŸã¯æ—¢å­˜ã®ã‚‚ã®ã‚’èª­ã¿è¾¼ã¿ï¼‰
        vector_store = Chroma(
            collection_name=collection_name,
            embedding_function=embedding_function,
            persist_directory=persist_dir,
        )

        # ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ï¼ˆæ—¢å­˜ã®IDãŒã‚ã‚Œã°æ›´æ–°ã•ã‚Œã‚‹ï¼‰
        vector_store.add_texts(texts=documents, metadatas=metadatas, ids=ids)

        # Chroma 0.4.xä»¥é™ã§ã¯è‡ªå‹•æ°¸ç¶šåŒ–ã®ãŸã‚persist()ã¯ä¸è¦
        # vector_store.persist()  # å‰Šé™¤

        logger.info("ChromaDBã¸ã®ãƒ‡ãƒ¼ã‚¿æ ¼ç´ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚")
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å†…ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã‚’å–å¾—ï¼ˆå…¬é–‹APIã‚’ä½¿ç”¨ï¼‰
        try:
            collection = vector_store.get()
            doc_count = len(collection["documents"]) if collection["documents"] else 0
            logger.info(
                f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection_name}' ã«ã¯ç¾åœ¨ "
                f"{doc_count} ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã™ã€‚"
            )
        except Exception as e:
            logger.warning(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            logger.info("ChromaDBã¸ã®ãƒ‡ãƒ¼ã‚¿æ ¼ç´ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    except Exception as e:
        logger.error(
            f"ChromaDBã¸ã®ãƒ‡ãƒ¼ã‚¿æ ¼ç´ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}",
            exc_info=True,
        )


def build_vector_engine(
    persist_dir: str,
    collection: str,
    config: Config,
    similarity_top_k: int = 15,
):
    """
    æ—¢å­˜ã®ChromaDBæ°¸ç¶šåŒ–ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰LlamaIndexã®VectorQueryEngineã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
    """
    if not os.path.exists(persist_dir) or not os.listdir(persist_dir):
        logger.error(
            f"ChromaDBã®æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ç©ºã§ã™: {persist_dir}"
        )
        raise FileNotFoundError(
            "ChromaDBã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ãƒ‡ãƒ¼ã‚¿æ ¼ç´ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        )

    logger.info(
        (
            f"æ—¢å­˜ã®ChromaDBã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection}' ã‹ã‚‰"
            "VectorQueryEngineã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™..."
        )
    )

    # OpenAIã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’LlamaIndexã®Settingsã«è¨­å®š
    Settings.embed_model = OpenAIEmbedding(**config.llamaindex_embedding_config)

    client = chromadb.PersistentClient(path=persist_dir)
    chroma_collection = client.get_or_create_collection(collection)

    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)

    # æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰
    v_index = VectorStoreIndex.from_vector_store(
        vector_store,
        storage_context=storage_context,
    )

    logger.info("VectorQueryEngineã®æ§‹ç¯‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    return v_index.as_query_engine(similarity_top_k=similarity_top_k)


def build_graph_engine(config: Config):
    """
    æ—¢å­˜ã®Neo4jã‚°ãƒ©ãƒ•ã‹ã‚‰LlamaIndexã®PropertyGraphQueryEngineã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
    APOCãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚
    """
    uri = config.neo4j_uri
    user = config.neo4j_user
    password = config.neo4j_password
    db_name = config.neo4j_database

    if not all([uri, user, password, db_name]):
        logger.error("Neo4jæ¥ç¶šæƒ…å ±ãŒ config ã‹ã‚‰å–å¾—ã§ãã¾ã›ã‚“ã€‚")
        raise ValueError(
            (
                "config ã« neo4j_uri, neo4j_user, neo4j_password, "
                "neo4j_database ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
            )
        )

    logger.info(
        (
            f"æ—¢å­˜ã®Neo4jã‚°ãƒ©ãƒ• '{db_name}' ã‹ã‚‰"
            "PropertyGraphQueryEngineã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™..."
        )
    )

    # OpenAIã®LLMã¨åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’LlamaIndexã®Settingsã«è¨­å®šï¼ˆconfigã‹ã‚‰ï¼‰
    Settings.llm = OpenAI(**config.llamaindex_llm_config)
    Settings.embed_model = OpenAIEmbedding(**config.llamaindex_embedding_config)

    try:
        # æ¨™æº–çš„ãªNeo4jPropertyGraphStoreã‚’ä½¿ç”¨ï¼ˆAPOCãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãŒå¿…è¦ï¼‰
        # ã“ã“ã§ã¯ç’°å¢ƒå¤‰æ•°ã®å­˜åœ¨ã‚’æ—¢ã«æ¤œè¨¼æ¸ˆã¿ã ãŒã€å‹ã‚·ã‚°ãƒãƒãƒ£ãŒ str ã‚’è¦æ±‚ã™ã‚‹ãŸã‚ã‚­ãƒ£ã‚¹ãƒˆ
        assert (
            user is not None
            and password is not None
            and uri is not None
            and db_name is not None
        )
        graph_store = Neo4jPropertyGraphStore(
            username=str(user),
            password=str(password),
            url=str(uri),
            database=str(db_name),
        )

        # æ—¢å­˜ã®ã‚°ãƒ©ãƒ•æ§‹é€ ã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰
        g_index = PropertyGraphIndex.from_existing(
            property_graph_store=graph_store,
        )

        logger.info("PropertyGraphQueryEngineã®æ§‹ç¯‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        return g_index.as_query_engine()

    except Exception as e:
        logger.error(f"Neo4jã‚°ãƒ©ãƒ•ã‚¨ãƒ³ã‚¸ãƒ³ã®æ§‹ç¯‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logger.error("APOCãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        logger.error(
            (
                f"Neo4jã‚°ãƒ©ãƒ•ã‚¨ãƒ³ã‚¸ãƒ³ã®æ§‹ç¯‰ä¸­ã«ä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸ "
                f"[{type(e).__name__}]: {e}\n"
                "å¤±æ•—ã—ãŸã‚¹ãƒ†ãƒƒãƒ—: Neo4jPropertyGraphStoreã®åˆæœŸåŒ–ã¾ãŸã¯"
                "PropertyGraphIndexã®ãƒ­ãƒ¼ãƒ‰ã€‚\n"
                "APOCãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            )
        )
        raise
